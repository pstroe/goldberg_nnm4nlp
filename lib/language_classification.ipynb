{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Class Classification - Language Classification\n",
    "\n",
    "This notebook implements the method presented in Goldberg's [2017] book \"Neural Network Methods for Natural Language Processing\". It shows the steps you need to go through in order to successfully train a classifier, and it should also, so I hope, illustrate the notational differences between Goldberg and standard machine learning literature.\n",
    "\n",
    "$NOTE$: There is no cross-validation etc. to find optimal parameters. This is simply to show how multi-class classification works. This will be part of a tutorial session and all other concepts will be explained there.\n",
    "\n",
    "Author: Phillip Ströbel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting and cleaning the data\n",
    "\n",
    "The data consists of downloaded Wikipedia articles (see `urls.txt`) in German, English, French, Spanish, Italian and Finnish (instead of \"O\" in Goldberg). The data is in HTML, so we need to some preprocessing to get the text out of it. We also restrict ourselfes to the characters from a to z in the alphabet (as described in Goldberg). In this fashion, we get rid of all the Umlauts (ä, ö, ü) and all other characters with diacritics (as, e.g., the é or ç in French). Note however, that if these characters ocurring in bigrams would probably be good features. In some way, we still keep the information \"special character\" by not fully deleting the character, but by replacing it by the dollar sign \"\\$\". Furthermore, we replace all punctuation marks and digits by dollar signs as well. As such, all special characters, digits, and punctuation marks are mapped to $. The space will be replaced by an underscore \"\\_\". We then represent each langauge by 28 characters, as is suggested by Goldberg."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning HTML\n",
    "We first strip the HTML to get only the text of the Wikipedia page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the html files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "from collections import defaultdict\n",
    "\n",
    "seed = np.random.seed(seed=200)  # set a seed for random, so results are reproducible\n",
    "\n",
    "article_dict = defaultdict(lambda: defaultdict(str))\n",
    "\n",
    "regex = r'[\\n ]{2,}'\n",
    "pattern = re.compile(regex)\n",
    "\n",
    "urls = open('urls.txt', 'r').readlines()\n",
    "\n",
    "for index, url in enumerate(urls):\n",
    "    language = url[8:10]\n",
    "    doc_id = 'doc_%d' % index\n",
    "    html = urlopen(url.strip()).read()    \n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    raw = soup.body.get_text()  # only get text from the text body (this excludes headers and should exclude navigation bars)\n",
    "    raw = re.sub(pattern, ' ', raw)  # replace multiple breaks and spaces by only one space\n",
    "    raw = re.sub(r'\\n', ' ', raw)  # replace every line break with a space\n",
    "    article_dict[language][doc_id] = raw.lower()  # assign each text to its language and lower all uppercase characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing --> prepare the text\n",
    "replace special characters and digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_dict = defaultdict(lambda: defaultdict(str))\n",
    "\n",
    "abc = r'[a-z]'\n",
    "abc_pattern = re.compile(abc)\n",
    "\n",
    "for lang, doc in article_dict.items():\n",
    "    for doc, text in doc.items():\n",
    "        for char in text:\n",
    "            if re.match(abc_pattern, char):\n",
    "                preprocessed_dict[lang][doc] += char\n",
    "            elif re.match(' ', char):\n",
    "                preprocessed_dict[lang][doc] += '_'\n",
    "            else:\n",
    "                preprocessed_dict[lang][doc] += '$'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count bigrams --> Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of bigrams will be our only feature. We could extend this by taking into account other n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "charset = 'abcdefghijklmnopqrstuvwxyz$_'  # define the character set we want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations_with_replacement, permutations\n",
    "\n",
    "def bigrams(text):\n",
    "    \"\"\"\n",
    "    Function to extract bigrams from text and calculate their distribution\n",
    "    :param text: text string\n",
    "    :return: dictionary containing bigrams as keys, and the normalised count as values\n",
    "    \"\"\"\n",
    "    combs = combinations_with_replacement(charset, 2)\n",
    "    perms = permutations(charset, 2)\n",
    "    bigram_dict = dict()\n",
    "    \n",
    "    for comb in set(list(combs) + list(perms)):\n",
    "        bigram_dict[''.join(comb)] = 0\n",
    "        \n",
    "    doc_length = len(text)\n",
    "    \n",
    "    for index in range(0, len(text)-1):\n",
    "        bigram = text[index] + text[index+1]\n",
    "        bigram_dict[bigram] += 1\n",
    "                \n",
    "    for bigram, count in bigram_dict.items():\n",
    "        bigram_dict[bigram] = count/doc_length\n",
    "\n",
    "    return bigram_dict              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put data into pandas dataframe\n",
    "The pandas dataframe allows us to conveniently represent all the data we need in one table. So let's do this. But first we need to extract the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_dict_full = defaultdict(lambda: defaultdict(dict))\n",
    "\n",
    "for lang, doc in preprocessed_dict.items():\n",
    "    for doc, text in sorted(doc.items()):\n",
    "        bigram_dict = bigrams(text)\n",
    "        bigram_dict_full[lang][doc] = bigram_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>$$</th>\n",
       "      <th>$_</th>\n",
       "      <th>$a</th>\n",
       "      <th>$b</th>\n",
       "      <th>$c</th>\n",
       "      <th>$d</th>\n",
       "      <th>$e</th>\n",
       "      <th>$f</th>\n",
       "      <th>$g</th>\n",
       "      <th>$h</th>\n",
       "      <th>...</th>\n",
       "      <th>zq</th>\n",
       "      <th>zr</th>\n",
       "      <th>zs</th>\n",
       "      <th>zt</th>\n",
       "      <th>zu</th>\n",
       "      <th>zv</th>\n",
       "      <th>zw</th>\n",
       "      <th>zx</th>\n",
       "      <th>zy</th>\n",
       "      <th>zz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.100732</td>\n",
       "      <td>0.036889</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.000332</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.062187</td>\n",
       "      <td>0.028911</td>\n",
       "      <td>0.000513</td>\n",
       "      <td>0.000322</td>\n",
       "      <td>0.000513</td>\n",
       "      <td>0.000356</td>\n",
       "      <td>0.000629</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.072008</td>\n",
       "      <td>0.031248</td>\n",
       "      <td>0.000444</td>\n",
       "      <td>0.000519</td>\n",
       "      <td>0.000497</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.075458</td>\n",
       "      <td>0.035249</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.000453</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.064235</td>\n",
       "      <td>0.032112</td>\n",
       "      <td>0.000344</td>\n",
       "      <td>0.000301</td>\n",
       "      <td>0.000735</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.000313</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         $$        $_        $a        $b        $c        $d        $e  \\\n",
       "0  0.100732  0.036889  0.000292  0.000222  0.000332  0.000163  0.000239   \n",
       "1  0.062187  0.028911  0.000513  0.000322  0.000513  0.000356  0.000629   \n",
       "2  0.072008  0.031248  0.000444  0.000519  0.000497  0.000187  0.000340   \n",
       "3  0.075458  0.035249  0.000400  0.000466  0.000453  0.000268  0.000303   \n",
       "4  0.064235  0.032112  0.000344  0.000301  0.000735  0.000295  0.000476   \n",
       "\n",
       "         $f        $g        $h    ...           zq        zr        zs   zt  \\\n",
       "0  0.000181  0.000099  0.000152    ...     0.000000  0.000006  0.000006  0.0   \n",
       "1  0.000192  0.000185  0.000171    ...     0.000007  0.000007  0.000000  0.0   \n",
       "2  0.000246  0.000093  0.000179    ...     0.000000  0.000004  0.000000  0.0   \n",
       "3  0.000224  0.000312  0.000189    ...     0.000000  0.000004  0.000000  0.0   \n",
       "4  0.000313  0.000271  0.000187    ...     0.000000  0.000000  0.000006  0.0   \n",
       "\n",
       "         zu        zv        zw   zx        zy        zz  \n",
       "0  0.000023  0.000000  0.000000  0.0  0.000000  0.000006  \n",
       "1  0.000027  0.000000  0.000000  0.0  0.000014  0.000021  \n",
       "2  0.000030  0.000004  0.000004  0.0  0.000011  0.000000  \n",
       "3  0.000013  0.000000  0.000000  0.0  0.000009  0.000000  \n",
       "4  0.000006  0.000000  0.000000  0.0  0.000000  0.000000  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "col_names = ['y'] + sorted(bigram_dict_full['en']['doc_0'].keys())\n",
    "my_df = dict()\n",
    "\n",
    "for col in col_names:\n",
    "    my_df[col] = list()\n",
    "    \n",
    "df = pd.DataFrame(my_df)\n",
    "\n",
    "for lang, doc in bigram_dict_full.items():\n",
    "    for key, value in doc.items():\n",
    "        df_obj = value\n",
    "        df_obj['y'] = lang\n",
    "        df = df.append(df_obj, ignore_index=True)\n",
    "        \n",
    "df.head()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 785)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split the data into the label vector \\begin{equation}\\mathbf{y}\\end{equation} and a training data matrix \\begin{equation}\\mathbf{X}\\end{equation}. But first, we shuffle the df and split it into a training and a test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, it is necessary for many machine learning tasks to standardise the data. Our aim is for each feature to be represented by a column vector in which values have zero mean and unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_point(datapoint, mean, std):\n",
    "    \"\"\"\n",
    "    normalise a datapoint to zero mean and unit variance.\n",
    "    :param datapoint: value as a float\n",
    "    :param mean: mean of data vector x\n",
    "    :param std: standard deviation of data vector x\n",
    "    :return: normalised datapoint (float)\n",
    "    \"\"\"\n",
    "    return (datapoint - mean)/std\n",
    "\n",
    "def normalise_matrix(matrix):\n",
    "    \"\"\"\n",
    "    normalises the data matrix\n",
    "    :param matrix: input matrix\n",
    "    :return: normalised matrix\n",
    "    \"\"\"\n",
    "    train_normalised = matrix.copy()\n",
    "    \n",
    "    for col in matrix:\n",
    "        try:\n",
    "            mean = matrix[col].mean()\n",
    "            std = matrix[col].std()\n",
    "            for index, item in enumerate(matrix[col]):\n",
    "                train_normalised.loc[index, col] = normalise_point(item, mean, std)\n",
    "        except ZeroDivisionError:\n",
    "            train_normalised.loc[index, col] = 0.0\n",
    "        except TypeError:\n",
    "            continue\n",
    "    return train_normalised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm = normalise_matrix(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into a train set and a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df_norm.sample(frac=0.9, random_state=seed)\n",
    "test = df_norm.drop(train.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the different sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for training\n",
    "y_train = train.y\n",
    "X_train = train.drop('y', axis=1)\n",
    "\n",
    "# for testing\n",
    "y_test = test.y\n",
    "X_test = test.drop('y', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples shape:  (54, 784)\n",
      "Training labels shape:  (54,)\n",
      "Test samples shape:  (6, 784)\n",
      "Test labels shape:  (6,)\n"
     ]
    }
   ],
   "source": [
    "print('Training samples shape: ', X_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Test samples shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should binarise our labels, although libraries like sklearn can also deal with non-numeric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(['en', 'fr', 'de', 'it', 'es', 'fi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['de', 'en', 'es', 'fi', 'fr', 'it'], dtype='<U2')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do this for both our training and test labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = lb.transform(y_train)\n",
    "y_test = lb.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels are now one-hot encoded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We almost have everything now. However, we need to take care of the bias and the weight matrix. The hypothesis ŷ is given by:\n",
    "\\begin{equation}\n",
    "\\mathbf{\\hat{y}}=\\mathbf{x}\\cdot\\mathbf{W}+\\mathbf{b}\n",
    "\\end{equation}\n",
    "We can achieve this by appending 1 to each feature vector x, and the whole weight vector b to the weight matrix W. This is called the bias trick. Note that the dimensions of X_train change, and that the weight matrix W will have match the dimensions (same number of rows as X has columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>$$</th>\n",
       "      <th>$_</th>\n",
       "      <th>$a</th>\n",
       "      <th>$b</th>\n",
       "      <th>$c</th>\n",
       "      <th>$d</th>\n",
       "      <th>$e</th>\n",
       "      <th>$f</th>\n",
       "      <th>$g</th>\n",
       "      <th>$h</th>\n",
       "      <th>...</th>\n",
       "      <th>zr</th>\n",
       "      <th>zs</th>\n",
       "      <th>zt</th>\n",
       "      <th>zu</th>\n",
       "      <th>zv</th>\n",
       "      <th>zw</th>\n",
       "      <th>zx</th>\n",
       "      <th>zy</th>\n",
       "      <th>zz</th>\n",
       "      <th>bias</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.416581</td>\n",
       "      <td>0.724355</td>\n",
       "      <td>0.415723</td>\n",
       "      <td>-0.678734</td>\n",
       "      <td>-0.973332</td>\n",
       "      <td>-0.650595</td>\n",
       "      <td>-0.396620</td>\n",
       "      <td>-0.943649</td>\n",
       "      <td>-0.864329</td>\n",
       "      <td>0.840628</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.543066</td>\n",
       "      <td>-0.498601</td>\n",
       "      <td>-0.428053</td>\n",
       "      <td>-0.440921</td>\n",
       "      <td>-0.281797</td>\n",
       "      <td>-0.418054</td>\n",
       "      <td>-0.275911</td>\n",
       "      <td>-0.448893</td>\n",
       "      <td>-0.452825</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.120851</td>\n",
       "      <td>0.754248</td>\n",
       "      <td>-0.134649</td>\n",
       "      <td>-0.331625</td>\n",
       "      <td>-0.658614</td>\n",
       "      <td>-0.085545</td>\n",
       "      <td>-0.823966</td>\n",
       "      <td>-0.596442</td>\n",
       "      <td>-0.633240</td>\n",
       "      <td>-0.321978</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.543066</td>\n",
       "      <td>-0.498601</td>\n",
       "      <td>-0.335196</td>\n",
       "      <td>-0.443902</td>\n",
       "      <td>-0.281797</td>\n",
       "      <td>-0.418054</td>\n",
       "      <td>-0.275911</td>\n",
       "      <td>0.247155</td>\n",
       "      <td>-0.427383</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>-0.325331</td>\n",
       "      <td>-0.585072</td>\n",
       "      <td>-1.236515</td>\n",
       "      <td>1.877263</td>\n",
       "      <td>-0.211181</td>\n",
       "      <td>-0.116645</td>\n",
       "      <td>-0.431707</td>\n",
       "      <td>-0.030080</td>\n",
       "      <td>0.331097</td>\n",
       "      <td>0.853819</td>\n",
       "      <td>...</td>\n",
       "      <td>0.710842</td>\n",
       "      <td>1.221637</td>\n",
       "      <td>2.202934</td>\n",
       "      <td>2.650994</td>\n",
       "      <td>-0.281797</td>\n",
       "      <td>2.054151</td>\n",
       "      <td>-0.275911</td>\n",
       "      <td>0.646756</td>\n",
       "      <td>-0.332681</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>-1.079756</td>\n",
       "      <td>0.965223</td>\n",
       "      <td>0.604572</td>\n",
       "      <td>-0.770761</td>\n",
       "      <td>-0.413110</td>\n",
       "      <td>-0.474208</td>\n",
       "      <td>-0.162656</td>\n",
       "      <td>-0.937676</td>\n",
       "      <td>-0.877084</td>\n",
       "      <td>-1.127069</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.543066</td>\n",
       "      <td>-0.498601</td>\n",
       "      <td>-0.428053</td>\n",
       "      <td>-0.453571</td>\n",
       "      <td>-0.281797</td>\n",
       "      <td>-0.418054</td>\n",
       "      <td>-0.275911</td>\n",
       "      <td>-0.448893</td>\n",
       "      <td>1.422711</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>-1.162418</td>\n",
       "      <td>-1.480115</td>\n",
       "      <td>1.662141</td>\n",
       "      <td>-0.692058</td>\n",
       "      <td>0.422829</td>\n",
       "      <td>-0.342705</td>\n",
       "      <td>-0.757610</td>\n",
       "      <td>4.230831</td>\n",
       "      <td>1.265686</td>\n",
       "      <td>-1.002417</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.543066</td>\n",
       "      <td>-0.498601</td>\n",
       "      <td>-0.428053</td>\n",
       "      <td>-0.412307</td>\n",
       "      <td>-0.281797</td>\n",
       "      <td>-0.418054</td>\n",
       "      <td>-0.275911</td>\n",
       "      <td>-0.448893</td>\n",
       "      <td>-0.452825</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.479922</td>\n",
       "      <td>-0.069024</td>\n",
       "      <td>-1.016872</td>\n",
       "      <td>-0.317584</td>\n",
       "      <td>-0.651994</td>\n",
       "      <td>-0.889686</td>\n",
       "      <td>-0.865250</td>\n",
       "      <td>-0.604430</td>\n",
       "      <td>-0.915983</td>\n",
       "      <td>-0.875618</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.089210</td>\n",
       "      <td>-0.498601</td>\n",
       "      <td>-0.428053</td>\n",
       "      <td>-0.433828</td>\n",
       "      <td>0.414915</td>\n",
       "      <td>-0.404073</td>\n",
       "      <td>-0.275911</td>\n",
       "      <td>-0.052319</td>\n",
       "      <td>-0.452825</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>-0.419403</td>\n",
       "      <td>-0.629671</td>\n",
       "      <td>-0.245953</td>\n",
       "      <td>0.937869</td>\n",
       "      <td>-0.465707</td>\n",
       "      <td>0.939226</td>\n",
       "      <td>-0.617174</td>\n",
       "      <td>0.122534</td>\n",
       "      <td>0.615506</td>\n",
       "      <td>1.581265</td>\n",
       "      <td>...</td>\n",
       "      <td>2.391855</td>\n",
       "      <td>0.508005</td>\n",
       "      <td>1.453602</td>\n",
       "      <td>2.812156</td>\n",
       "      <td>1.970898</td>\n",
       "      <td>1.932708</td>\n",
       "      <td>-0.275911</td>\n",
       "      <td>1.260772</td>\n",
       "      <td>-0.405957</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.084788</td>\n",
       "      <td>-0.117571</td>\n",
       "      <td>0.370100</td>\n",
       "      <td>-0.561941</td>\n",
       "      <td>0.821552</td>\n",
       "      <td>-0.558541</td>\n",
       "      <td>0.326848</td>\n",
       "      <td>-0.428954</td>\n",
       "      <td>0.236760</td>\n",
       "      <td>-0.813851</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.543066</td>\n",
       "      <td>0.051543</td>\n",
       "      <td>-0.428053</td>\n",
       "      <td>-0.450279</td>\n",
       "      <td>-0.281797</td>\n",
       "      <td>-0.418054</td>\n",
       "      <td>-0.275911</td>\n",
       "      <td>-0.448893</td>\n",
       "      <td>-0.375979</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>-0.870141</td>\n",
       "      <td>1.050332</td>\n",
       "      <td>0.771331</td>\n",
       "      <td>-0.120226</td>\n",
       "      <td>-0.119737</td>\n",
       "      <td>0.866913</td>\n",
       "      <td>-0.414991</td>\n",
       "      <td>-0.356353</td>\n",
       "      <td>-0.508410</td>\n",
       "      <td>-0.345873</td>\n",
       "      <td>...</td>\n",
       "      <td>0.986294</td>\n",
       "      <td>-0.498601</td>\n",
       "      <td>-0.428053</td>\n",
       "      <td>-0.438472</td>\n",
       "      <td>-0.281797</td>\n",
       "      <td>-0.418054</td>\n",
       "      <td>-0.275911</td>\n",
       "      <td>-0.003447</td>\n",
       "      <td>1.256774</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>-0.585015</td>\n",
       "      <td>-0.009095</td>\n",
       "      <td>-0.342072</td>\n",
       "      <td>-0.744674</td>\n",
       "      <td>-0.604092</td>\n",
       "      <td>-0.743825</td>\n",
       "      <td>-0.442110</td>\n",
       "      <td>-0.326296</td>\n",
       "      <td>-0.867174</td>\n",
       "      <td>-1.005133</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.543066</td>\n",
       "      <td>-0.498601</td>\n",
       "      <td>-0.428053</td>\n",
       "      <td>-0.463298</td>\n",
       "      <td>-0.281797</td>\n",
       "      <td>-0.418054</td>\n",
       "      <td>2.072622</td>\n",
       "      <td>-0.448893</td>\n",
       "      <td>4.015987</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.160193</td>\n",
       "      <td>-0.511410</td>\n",
       "      <td>1.069538</td>\n",
       "      <td>0.386715</td>\n",
       "      <td>1.337398</td>\n",
       "      <td>-0.555819</td>\n",
       "      <td>0.504810</td>\n",
       "      <td>0.818337</td>\n",
       "      <td>-0.460647</td>\n",
       "      <td>-0.895661</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.543066</td>\n",
       "      <td>-0.498601</td>\n",
       "      <td>-0.428053</td>\n",
       "      <td>-0.296696</td>\n",
       "      <td>-0.281797</td>\n",
       "      <td>-0.418054</td>\n",
       "      <td>-0.275911</td>\n",
       "      <td>-0.448893</td>\n",
       "      <td>-0.452825</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>-0.607005</td>\n",
       "      <td>-0.789282</td>\n",
       "      <td>-0.789932</td>\n",
       "      <td>1.403177</td>\n",
       "      <td>0.105876</td>\n",
       "      <td>0.858891</td>\n",
       "      <td>-0.563635</td>\n",
       "      <td>0.437168</td>\n",
       "      <td>0.305471</td>\n",
       "      <td>1.246306</td>\n",
       "      <td>...</td>\n",
       "      <td>0.308898</td>\n",
       "      <td>1.839021</td>\n",
       "      <td>3.916848</td>\n",
       "      <td>2.721069</td>\n",
       "      <td>1.026050</td>\n",
       "      <td>4.726126</td>\n",
       "      <td>-0.275911</td>\n",
       "      <td>0.295542</td>\n",
       "      <td>-0.384799</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>-0.731713</td>\n",
       "      <td>-1.069508</td>\n",
       "      <td>-0.770870</td>\n",
       "      <td>0.943741</td>\n",
       "      <td>-0.165573</td>\n",
       "      <td>1.359172</td>\n",
       "      <td>0.163119</td>\n",
       "      <td>0.289471</td>\n",
       "      <td>0.656157</td>\n",
       "      <td>3.330577</td>\n",
       "      <td>...</td>\n",
       "      <td>1.708625</td>\n",
       "      <td>1.045949</td>\n",
       "      <td>1.278041</td>\n",
       "      <td>1.748099</td>\n",
       "      <td>-0.281797</td>\n",
       "      <td>1.940397</td>\n",
       "      <td>-0.275911</td>\n",
       "      <td>2.830273</td>\n",
       "      <td>-0.237077</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.374859</td>\n",
       "      <td>0.792281</td>\n",
       "      <td>-0.885054</td>\n",
       "      <td>-0.102468</td>\n",
       "      <td>-0.338211</td>\n",
       "      <td>-0.562495</td>\n",
       "      <td>-0.732605</td>\n",
       "      <td>2.286174</td>\n",
       "      <td>-0.509960</td>\n",
       "      <td>-0.707194</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.543066</td>\n",
       "      <td>-0.498601</td>\n",
       "      <td>-0.428053</td>\n",
       "      <td>-0.365732</td>\n",
       "      <td>1.137658</td>\n",
       "      <td>-0.418054</td>\n",
       "      <td>1.633115</td>\n",
       "      <td>-0.179572</td>\n",
       "      <td>-0.423293</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.471188</td>\n",
       "      <td>0.406604</td>\n",
       "      <td>0.773766</td>\n",
       "      <td>-0.493871</td>\n",
       "      <td>0.056359</td>\n",
       "      <td>-0.479218</td>\n",
       "      <td>-0.111865</td>\n",
       "      <td>0.779181</td>\n",
       "      <td>-0.428327</td>\n",
       "      <td>-0.742011</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.543066</td>\n",
       "      <td>-0.498601</td>\n",
       "      <td>-0.428053</td>\n",
       "      <td>-0.422477</td>\n",
       "      <td>-0.281797</td>\n",
       "      <td>-0.418054</td>\n",
       "      <td>-0.275911</td>\n",
       "      <td>-0.448893</td>\n",
       "      <td>-0.399281</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>-0.130612</td>\n",
       "      <td>-0.433647</td>\n",
       "      <td>-0.906393</td>\n",
       "      <td>2.334391</td>\n",
       "      <td>-0.069298</td>\n",
       "      <td>1.575522</td>\n",
       "      <td>-0.336380</td>\n",
       "      <td>0.797473</td>\n",
       "      <td>0.605589</td>\n",
       "      <td>1.578526</td>\n",
       "      <td>...</td>\n",
       "      <td>2.401255</td>\n",
       "      <td>4.550550</td>\n",
       "      <td>1.459628</td>\n",
       "      <td>1.568013</td>\n",
       "      <td>3.108068</td>\n",
       "      <td>1.668127</td>\n",
       "      <td>-0.275911</td>\n",
       "      <td>-0.020107</td>\n",
       "      <td>-0.382297</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.472022</td>\n",
       "      <td>0.478789</td>\n",
       "      <td>-0.955261</td>\n",
       "      <td>1.189247</td>\n",
       "      <td>-0.155047</td>\n",
       "      <td>0.017002</td>\n",
       "      <td>-0.534238</td>\n",
       "      <td>-0.164209</td>\n",
       "      <td>-0.110410</td>\n",
       "      <td>-0.500498</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.543066</td>\n",
       "      <td>1.575044</td>\n",
       "      <td>-0.369321</td>\n",
       "      <td>-0.414225</td>\n",
       "      <td>-0.281797</td>\n",
       "      <td>-0.262841</td>\n",
       "      <td>-0.275911</td>\n",
       "      <td>-0.448893</td>\n",
       "      <td>-0.388458</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.212116</td>\n",
       "      <td>-0.388959</td>\n",
       "      <td>-1.012269</td>\n",
       "      <td>1.266830</td>\n",
       "      <td>-0.138706</td>\n",
       "      <td>0.496486</td>\n",
       "      <td>-0.560331</td>\n",
       "      <td>0.162889</td>\n",
       "      <td>0.699592</td>\n",
       "      <td>1.402485</td>\n",
       "      <td>...</td>\n",
       "      <td>1.341123</td>\n",
       "      <td>0.147630</td>\n",
       "      <td>2.372317</td>\n",
       "      <td>2.197710</td>\n",
       "      <td>1.164410</td>\n",
       "      <td>0.800887</td>\n",
       "      <td>-0.275911</td>\n",
       "      <td>0.648694</td>\n",
       "      <td>-0.392647</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.242526</td>\n",
       "      <td>-0.034328</td>\n",
       "      <td>-0.869410</td>\n",
       "      <td>-0.778517</td>\n",
       "      <td>-1.070747</td>\n",
       "      <td>-0.594863</td>\n",
       "      <td>-0.658519</td>\n",
       "      <td>-0.825305</td>\n",
       "      <td>-1.081411</td>\n",
       "      <td>1.818419</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.543066</td>\n",
       "      <td>-0.498601</td>\n",
       "      <td>-0.428053</td>\n",
       "      <td>-0.463298</td>\n",
       "      <td>-0.281797</td>\n",
       "      <td>-0.418054</td>\n",
       "      <td>-0.275911</td>\n",
       "      <td>-0.448893</td>\n",
       "      <td>-0.452825</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.142429</td>\n",
       "      <td>0.238991</td>\n",
       "      <td>0.939711</td>\n",
       "      <td>1.545391</td>\n",
       "      <td>0.998756</td>\n",
       "      <td>3.126071</td>\n",
       "      <td>1.689346</td>\n",
       "      <td>0.143515</td>\n",
       "      <td>2.817628</td>\n",
       "      <td>0.960311</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.543066</td>\n",
       "      <td>-0.498601</td>\n",
       "      <td>-0.428053</td>\n",
       "      <td>-0.404733</td>\n",
       "      <td>-0.281797</td>\n",
       "      <td>-0.362484</td>\n",
       "      <td>-0.275911</td>\n",
       "      <td>-0.448893</td>\n",
       "      <td>-0.452825</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.146130</td>\n",
       "      <td>1.348289</td>\n",
       "      <td>1.810425</td>\n",
       "      <td>-0.026592</td>\n",
       "      <td>2.606196</td>\n",
       "      <td>2.133229</td>\n",
       "      <td>1.099025</td>\n",
       "      <td>2.254320</td>\n",
       "      <td>1.173588</td>\n",
       "      <td>0.034305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.750949</td>\n",
       "      <td>-0.498601</td>\n",
       "      <td>-0.428053</td>\n",
       "      <td>-0.442293</td>\n",
       "      <td>-0.281797</td>\n",
       "      <td>-0.418054</td>\n",
       "      <td>-0.275911</td>\n",
       "      <td>-0.448893</td>\n",
       "      <td>-0.060199</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>-0.702308</td>\n",
       "      <td>-0.345426</td>\n",
       "      <td>-0.596958</td>\n",
       "      <td>-0.645196</td>\n",
       "      <td>-0.335803</td>\n",
       "      <td>-0.608656</td>\n",
       "      <td>-0.343109</td>\n",
       "      <td>-0.259409</td>\n",
       "      <td>-0.810021</td>\n",
       "      <td>-0.918792</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.543066</td>\n",
       "      <td>-0.498601</td>\n",
       "      <td>-0.428053</td>\n",
       "      <td>-0.463298</td>\n",
       "      <td>-0.281797</td>\n",
       "      <td>-0.418054</td>\n",
       "      <td>-0.275911</td>\n",
       "      <td>-0.448893</td>\n",
       "      <td>1.200665</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-0.985567</td>\n",
       "      <td>-0.626714</td>\n",
       "      <td>1.241368</td>\n",
       "      <td>-0.733494</td>\n",
       "      <td>1.978165</td>\n",
       "      <td>1.399586</td>\n",
       "      <td>1.333401</td>\n",
       "      <td>0.696612</td>\n",
       "      <td>1.197366</td>\n",
       "      <td>0.443793</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.543066</td>\n",
       "      <td>-0.498601</td>\n",
       "      <td>-0.428053</td>\n",
       "      <td>-0.463298</td>\n",
       "      <td>-0.281797</td>\n",
       "      <td>-0.418054</td>\n",
       "      <td>-0.275911</td>\n",
       "      <td>-0.448893</td>\n",
       "      <td>-0.452825</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>-0.192614</td>\n",
       "      <td>-1.926658</td>\n",
       "      <td>-0.985484</td>\n",
       "      <td>2.868378</td>\n",
       "      <td>-0.495550</td>\n",
       "      <td>-0.317526</td>\n",
       "      <td>-0.415041</td>\n",
       "      <td>2.220213</td>\n",
       "      <td>1.314527</td>\n",
       "      <td>0.568383</td>\n",
       "      <td>...</td>\n",
       "      <td>3.622817</td>\n",
       "      <td>2.358992</td>\n",
       "      <td>2.242804</td>\n",
       "      <td>2.816532</td>\n",
       "      <td>6.113241</td>\n",
       "      <td>1.635311</td>\n",
       "      <td>-0.275911</td>\n",
       "      <td>1.977838</td>\n",
       "      <td>-0.319773</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.513664</td>\n",
       "      <td>0.665937</td>\n",
       "      <td>-0.711762</td>\n",
       "      <td>-0.946432</td>\n",
       "      <td>-1.079040</td>\n",
       "      <td>-0.186931</td>\n",
       "      <td>-0.582588</td>\n",
       "      <td>-0.646024</td>\n",
       "      <td>-0.989753</td>\n",
       "      <td>0.730740</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.543066</td>\n",
       "      <td>0.580242</td>\n",
       "      <td>-0.428053</td>\n",
       "      <td>-0.463298</td>\n",
       "      <td>-0.281797</td>\n",
       "      <td>-0.418054</td>\n",
       "      <td>-0.275911</td>\n",
       "      <td>-0.448893</td>\n",
       "      <td>-0.452825</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.246974</td>\n",
       "      <td>1.116871</td>\n",
       "      <td>1.936067</td>\n",
       "      <td>-0.541777</td>\n",
       "      <td>1.698733</td>\n",
       "      <td>2.109133</td>\n",
       "      <td>1.623385</td>\n",
       "      <td>0.512615</td>\n",
       "      <td>0.969572</td>\n",
       "      <td>-0.167648</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.543066</td>\n",
       "      <td>-0.048653</td>\n",
       "      <td>-0.428053</td>\n",
       "      <td>-0.457974</td>\n",
       "      <td>-0.281797</td>\n",
       "      <td>-0.418054</td>\n",
       "      <td>-0.275911</td>\n",
       "      <td>-0.448893</td>\n",
       "      <td>-0.431875</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>-0.501468</td>\n",
       "      <td>0.004917</td>\n",
       "      <td>-0.115528</td>\n",
       "      <td>-0.897042</td>\n",
       "      <td>-0.396359</td>\n",
       "      <td>-0.834175</td>\n",
       "      <td>-0.436480</td>\n",
       "      <td>-0.779942</td>\n",
       "      <td>-0.916704</td>\n",
       "      <td>-0.769088</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.152984</td>\n",
       "      <td>-0.498601</td>\n",
       "      <td>-0.405317</td>\n",
       "      <td>-0.437970</td>\n",
       "      <td>-0.281797</td>\n",
       "      <td>-0.418054</td>\n",
       "      <td>-0.275911</td>\n",
       "      <td>-0.108045</td>\n",
       "      <td>1.316306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>-0.274015</td>\n",
       "      <td>-1.322232</td>\n",
       "      <td>-1.071332</td>\n",
       "      <td>2.319960</td>\n",
       "      <td>0.991819</td>\n",
       "      <td>0.615340</td>\n",
       "      <td>-0.273673</td>\n",
       "      <td>0.049977</td>\n",
       "      <td>0.551903</td>\n",
       "      <td>1.345466</td>\n",
       "      <td>...</td>\n",
       "      <td>2.558575</td>\n",
       "      <td>1.628973</td>\n",
       "      <td>2.645150</td>\n",
       "      <td>1.626200</td>\n",
       "      <td>-0.281797</td>\n",
       "      <td>1.301845</td>\n",
       "      <td>-0.275911</td>\n",
       "      <td>0.454499</td>\n",
       "      <td>-0.353763</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.606613</td>\n",
       "      <td>1.290868</td>\n",
       "      <td>-1.151992</td>\n",
       "      <td>-0.850743</td>\n",
       "      <td>-0.865548</td>\n",
       "      <td>-0.941448</td>\n",
       "      <td>-0.934557</td>\n",
       "      <td>-0.806296</td>\n",
       "      <td>-0.901088</td>\n",
       "      <td>-0.929057</td>\n",
       "      <td>...</td>\n",
       "      <td>0.165733</td>\n",
       "      <td>-0.012400</td>\n",
       "      <td>-0.428053</td>\n",
       "      <td>-0.440286</td>\n",
       "      <td>-0.281797</td>\n",
       "      <td>-0.418054</td>\n",
       "      <td>-0.275911</td>\n",
       "      <td>-0.448893</td>\n",
       "      <td>-0.430187</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.180615</td>\n",
       "      <td>0.166149</td>\n",
       "      <td>-0.930366</td>\n",
       "      <td>1.191977</td>\n",
       "      <td>-0.832784</td>\n",
       "      <td>-0.435478</td>\n",
       "      <td>-0.519567</td>\n",
       "      <td>-1.078332</td>\n",
       "      <td>-0.948434</td>\n",
       "      <td>0.920561</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.543066</td>\n",
       "      <td>-0.498601</td>\n",
       "      <td>-0.428053</td>\n",
       "      <td>-0.338119</td>\n",
       "      <td>-0.281797</td>\n",
       "      <td>-0.374862</td>\n",
       "      <td>-0.275911</td>\n",
       "      <td>-0.448893</td>\n",
       "      <td>-0.452825</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>-1.160164</td>\n",
       "      <td>-1.146230</td>\n",
       "      <td>-0.422590</td>\n",
       "      <td>1.920241</td>\n",
       "      <td>-0.209121</td>\n",
       "      <td>0.397336</td>\n",
       "      <td>-0.501577</td>\n",
       "      <td>-0.675719</td>\n",
       "      <td>1.334511</td>\n",
       "      <td>0.805749</td>\n",
       "      <td>...</td>\n",
       "      <td>0.210705</td>\n",
       "      <td>1.569597</td>\n",
       "      <td>2.032189</td>\n",
       "      <td>2.326522</td>\n",
       "      <td>-0.281797</td>\n",
       "      <td>2.066576</td>\n",
       "      <td>-0.275911</td>\n",
       "      <td>-0.009803</td>\n",
       "      <td>-0.356528</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.615245</td>\n",
       "      <td>0.895433</td>\n",
       "      <td>-1.055850</td>\n",
       "      <td>-0.412176</td>\n",
       "      <td>-0.708640</td>\n",
       "      <td>-0.709287</td>\n",
       "      <td>-0.890224</td>\n",
       "      <td>-0.672407</td>\n",
       "      <td>-0.352244</td>\n",
       "      <td>-0.856441</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008425</td>\n",
       "      <td>-0.498601</td>\n",
       "      <td>-0.428053</td>\n",
       "      <td>-0.450280</td>\n",
       "      <td>-0.281797</td>\n",
       "      <td>-0.418054</td>\n",
       "      <td>-0.275911</td>\n",
       "      <td>-0.137451</td>\n",
       "      <td>-0.452825</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>-1.131557</td>\n",
       "      <td>-1.189601</td>\n",
       "      <td>0.114841</td>\n",
       "      <td>-0.950014</td>\n",
       "      <td>-0.951953</td>\n",
       "      <td>-0.966449</td>\n",
       "      <td>-0.527125</td>\n",
       "      <td>-0.953242</td>\n",
       "      <td>-0.667235</td>\n",
       "      <td>-1.057044</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.543066</td>\n",
       "      <td>-0.102647</td>\n",
       "      <td>-0.428053</td>\n",
       "      <td>-0.458613</td>\n",
       "      <td>-0.281797</td>\n",
       "      <td>-0.418054</td>\n",
       "      <td>-0.275911</td>\n",
       "      <td>-0.280766</td>\n",
       "      <td>3.547799</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>-1.252685</td>\n",
       "      <td>-1.505715</td>\n",
       "      <td>1.496289</td>\n",
       "      <td>-0.804829</td>\n",
       "      <td>-0.440550</td>\n",
       "      <td>-0.607955</td>\n",
       "      <td>-0.882890</td>\n",
       "      <td>0.252852</td>\n",
       "      <td>-0.559621</td>\n",
       "      <td>-1.092905</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.543066</td>\n",
       "      <td>-0.069201</td>\n",
       "      <td>-0.428053</td>\n",
       "      <td>-0.382004</td>\n",
       "      <td>-0.281797</td>\n",
       "      <td>-0.398770</td>\n",
       "      <td>-0.275911</td>\n",
       "      <td>-0.448893</td>\n",
       "      <td>-0.452825</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5.740946</td>\n",
       "      <td>-0.227077</td>\n",
       "      <td>-0.032320</td>\n",
       "      <td>-0.797653</td>\n",
       "      <td>-0.938446</td>\n",
       "      <td>-1.135772</td>\n",
       "      <td>0.093860</td>\n",
       "      <td>-0.975889</td>\n",
       "      <td>-1.026972</td>\n",
       "      <td>1.888929</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.543066</td>\n",
       "      <td>-0.498601</td>\n",
       "      <td>-0.428053</td>\n",
       "      <td>-0.438515</td>\n",
       "      <td>-0.281797</td>\n",
       "      <td>-0.418054</td>\n",
       "      <td>-0.275911</td>\n",
       "      <td>-0.448893</td>\n",
       "      <td>-0.452825</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.700461</td>\n",
       "      <td>2.503420</td>\n",
       "      <td>0.690355</td>\n",
       "      <td>-0.316830</td>\n",
       "      <td>0.857275</td>\n",
       "      <td>0.392512</td>\n",
       "      <td>4.008051</td>\n",
       "      <td>2.059705</td>\n",
       "      <td>0.563204</td>\n",
       "      <td>0.596550</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.543066</td>\n",
       "      <td>-0.498601</td>\n",
       "      <td>-0.428053</td>\n",
       "      <td>-0.316883</td>\n",
       "      <td>-0.281797</td>\n",
       "      <td>-0.418054</td>\n",
       "      <td>3.862177</td>\n",
       "      <td>0.426795</td>\n",
       "      <td>-0.452825</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.076709</td>\n",
       "      <td>1.156331</td>\n",
       "      <td>-0.273990</td>\n",
       "      <td>-0.740141</td>\n",
       "      <td>-0.671521</td>\n",
       "      <td>-0.509198</td>\n",
       "      <td>-0.566176</td>\n",
       "      <td>-0.636286</td>\n",
       "      <td>-0.208105</td>\n",
       "      <td>-0.984900</td>\n",
       "      <td>...</td>\n",
       "      <td>0.604730</td>\n",
       "      <td>-0.498601</td>\n",
       "      <td>-0.428053</td>\n",
       "      <td>-0.435350</td>\n",
       "      <td>-0.281797</td>\n",
       "      <td>-0.418054</td>\n",
       "      <td>-0.275911</td>\n",
       "      <td>-0.114583</td>\n",
       "      <td>2.149968</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.794710</td>\n",
       "      <td>-0.443708</td>\n",
       "      <td>2.248452</td>\n",
       "      <td>-0.088203</td>\n",
       "      <td>1.191352</td>\n",
       "      <td>0.305211</td>\n",
       "      <td>1.907300</td>\n",
       "      <td>0.160505</td>\n",
       "      <td>1.267693</td>\n",
       "      <td>-0.307165</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.543066</td>\n",
       "      <td>-0.498601</td>\n",
       "      <td>-0.396443</td>\n",
       "      <td>-0.445691</td>\n",
       "      <td>-0.281797</td>\n",
       "      <td>-0.367932</td>\n",
       "      <td>-0.275911</td>\n",
       "      <td>-0.448893</td>\n",
       "      <td>-0.452825</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.414358</td>\n",
       "      <td>-0.442215</td>\n",
       "      <td>-1.126501</td>\n",
       "      <td>-1.001828</td>\n",
       "      <td>-0.762116</td>\n",
       "      <td>-0.290679</td>\n",
       "      <td>-0.280082</td>\n",
       "      <td>0.326404</td>\n",
       "      <td>-1.038486</td>\n",
       "      <td>-0.159550</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.543066</td>\n",
       "      <td>-0.498601</td>\n",
       "      <td>-0.428053</td>\n",
       "      <td>-0.463298</td>\n",
       "      <td>-0.281797</td>\n",
       "      <td>-0.418054</td>\n",
       "      <td>-0.275911</td>\n",
       "      <td>-0.448893</td>\n",
       "      <td>-0.452825</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.193774</td>\n",
       "      <td>1.395213</td>\n",
       "      <td>1.803825</td>\n",
       "      <td>-0.143628</td>\n",
       "      <td>2.903953</td>\n",
       "      <td>2.254735</td>\n",
       "      <td>3.313954</td>\n",
       "      <td>0.261266</td>\n",
       "      <td>2.307390</td>\n",
       "      <td>0.675373</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.543066</td>\n",
       "      <td>-0.498601</td>\n",
       "      <td>-0.428053</td>\n",
       "      <td>-0.450895</td>\n",
       "      <td>-0.281797</td>\n",
       "      <td>-0.347438</td>\n",
       "      <td>-0.275911</td>\n",
       "      <td>-0.226341</td>\n",
       "      <td>-0.404017</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.126604</td>\n",
       "      <td>0.753360</td>\n",
       "      <td>-0.034665</td>\n",
       "      <td>-0.962101</td>\n",
       "      <td>-0.234993</td>\n",
       "      <td>-0.729560</td>\n",
       "      <td>-0.152465</td>\n",
       "      <td>-1.055616</td>\n",
       "      <td>-0.489111</td>\n",
       "      <td>-0.836700</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.543066</td>\n",
       "      <td>-0.498601</td>\n",
       "      <td>-0.428053</td>\n",
       "      <td>-0.443632</td>\n",
       "      <td>-0.281797</td>\n",
       "      <td>-0.418054</td>\n",
       "      <td>-0.275911</td>\n",
       "      <td>-0.448893</td>\n",
       "      <td>1.172272</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>-1.058618</td>\n",
       "      <td>-1.816994</td>\n",
       "      <td>1.543473</td>\n",
       "      <td>-0.881029</td>\n",
       "      <td>-0.033405</td>\n",
       "      <td>-0.567117</td>\n",
       "      <td>-0.517968</td>\n",
       "      <td>-0.496778</td>\n",
       "      <td>-0.299887</td>\n",
       "      <td>-1.049161</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.543066</td>\n",
       "      <td>-0.498601</td>\n",
       "      <td>-0.428053</td>\n",
       "      <td>-0.438055</td>\n",
       "      <td>-0.281797</td>\n",
       "      <td>-0.418054</td>\n",
       "      <td>-0.275911</td>\n",
       "      <td>-0.448893</td>\n",
       "      <td>-0.452825</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.222231</td>\n",
       "      <td>1.226280</td>\n",
       "      <td>-1.005652</td>\n",
       "      <td>-0.721303</td>\n",
       "      <td>-0.194072</td>\n",
       "      <td>1.367404</td>\n",
       "      <td>-0.155472</td>\n",
       "      <td>0.343983</td>\n",
       "      <td>-0.063664</td>\n",
       "      <td>-0.527539</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.543066</td>\n",
       "      <td>-0.498601</td>\n",
       "      <td>-0.428053</td>\n",
       "      <td>-0.431093</td>\n",
       "      <td>-0.281797</td>\n",
       "      <td>-0.295818</td>\n",
       "      <td>-0.275911</td>\n",
       "      <td>1.862513</td>\n",
       "      <td>-0.452825</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>-0.353525</td>\n",
       "      <td>0.271025</td>\n",
       "      <td>-0.123316</td>\n",
       "      <td>0.288610</td>\n",
       "      <td>-0.183117</td>\n",
       "      <td>0.305245</td>\n",
       "      <td>-0.240993</td>\n",
       "      <td>-0.363879</td>\n",
       "      <td>-0.621295</td>\n",
       "      <td>-0.719784</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.543066</td>\n",
       "      <td>-0.498601</td>\n",
       "      <td>-0.428053</td>\n",
       "      <td>-0.360794</td>\n",
       "      <td>-0.281797</td>\n",
       "      <td>-0.369423</td>\n",
       "      <td>-0.275911</td>\n",
       "      <td>-0.448893</td>\n",
       "      <td>1.916884</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.164446</td>\n",
       "      <td>-0.233160</td>\n",
       "      <td>-1.025623</td>\n",
       "      <td>-0.627827</td>\n",
       "      <td>-0.649142</td>\n",
       "      <td>-0.875562</td>\n",
       "      <td>-0.861011</td>\n",
       "      <td>-0.743711</td>\n",
       "      <td>-0.700612</td>\n",
       "      <td>-0.661963</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.543066</td>\n",
       "      <td>-0.498601</td>\n",
       "      <td>-0.428053</td>\n",
       "      <td>-0.455361</td>\n",
       "      <td>-0.281797</td>\n",
       "      <td>-0.418054</td>\n",
       "      <td>-0.275911</td>\n",
       "      <td>-0.448893</td>\n",
       "      <td>-0.390360</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.094701</td>\n",
       "      <td>-0.632283</td>\n",
       "      <td>-0.955982</td>\n",
       "      <td>-0.671482</td>\n",
       "      <td>-0.630449</td>\n",
       "      <td>-0.515897</td>\n",
       "      <td>-0.665854</td>\n",
       "      <td>-0.773050</td>\n",
       "      <td>-0.680659</td>\n",
       "      <td>-0.891426</td>\n",
       "      <td>...</td>\n",
       "      <td>0.288623</td>\n",
       "      <td>-0.498601</td>\n",
       "      <td>-0.428053</td>\n",
       "      <td>-0.436297</td>\n",
       "      <td>-0.281797</td>\n",
       "      <td>-0.418054</td>\n",
       "      <td>-0.275911</td>\n",
       "      <td>0.035587</td>\n",
       "      <td>-0.373136</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>-0.988473</td>\n",
       "      <td>-1.213568</td>\n",
       "      <td>0.729079</td>\n",
       "      <td>-0.863401</td>\n",
       "      <td>-0.004365</td>\n",
       "      <td>-0.655731</td>\n",
       "      <td>-0.199782</td>\n",
       "      <td>-0.582613</td>\n",
       "      <td>-0.152036</td>\n",
       "      <td>-0.958272</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.543066</td>\n",
       "      <td>-0.498601</td>\n",
       "      <td>-0.428053</td>\n",
       "      <td>-0.424817</td>\n",
       "      <td>-0.281797</td>\n",
       "      <td>-0.418054</td>\n",
       "      <td>-0.275911</td>\n",
       "      <td>-0.448893</td>\n",
       "      <td>-0.452825</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.270592</td>\n",
       "      <td>-0.777777</td>\n",
       "      <td>-0.689856</td>\n",
       "      <td>-0.490257</td>\n",
       "      <td>-0.707406</td>\n",
       "      <td>-0.679490</td>\n",
       "      <td>-0.786643</td>\n",
       "      <td>-0.616063</td>\n",
       "      <td>-0.712942</td>\n",
       "      <td>-0.783167</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.543066</td>\n",
       "      <td>2.764593</td>\n",
       "      <td>-0.428053</td>\n",
       "      <td>-0.447854</td>\n",
       "      <td>-0.281797</td>\n",
       "      <td>-0.418054</td>\n",
       "      <td>-0.275911</td>\n",
       "      <td>0.105343</td>\n",
       "      <td>-0.422438</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.467117</td>\n",
       "      <td>-1.502584</td>\n",
       "      <td>-0.694044</td>\n",
       "      <td>-0.812725</td>\n",
       "      <td>-1.122372</td>\n",
       "      <td>-1.153273</td>\n",
       "      <td>-0.486382</td>\n",
       "      <td>-1.154678</td>\n",
       "      <td>-0.843757</td>\n",
       "      <td>0.422122</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.543066</td>\n",
       "      <td>-0.498601</td>\n",
       "      <td>-0.428053</td>\n",
       "      <td>-0.449991</td>\n",
       "      <td>-0.281797</td>\n",
       "      <td>-0.367546</td>\n",
       "      <td>-0.275911</td>\n",
       "      <td>-0.448893</td>\n",
       "      <td>-0.452825</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>-0.212479</td>\n",
       "      <td>-0.633552</td>\n",
       "      <td>1.046139</td>\n",
       "      <td>0.748302</td>\n",
       "      <td>0.244387</td>\n",
       "      <td>-0.215460</td>\n",
       "      <td>-0.173398</td>\n",
       "      <td>1.172185</td>\n",
       "      <td>0.146983</td>\n",
       "      <td>-0.746660</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.543066</td>\n",
       "      <td>-0.498601</td>\n",
       "      <td>-0.325594</td>\n",
       "      <td>-0.406225</td>\n",
       "      <td>-0.281797</td>\n",
       "      <td>-0.418054</td>\n",
       "      <td>-0.275911</td>\n",
       "      <td>-0.448893</td>\n",
       "      <td>-0.452825</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.784760</td>\n",
       "      <td>0.085900</td>\n",
       "      <td>1.812258</td>\n",
       "      <td>-0.292717</td>\n",
       "      <td>2.393790</td>\n",
       "      <td>0.475154</td>\n",
       "      <td>1.434863</td>\n",
       "      <td>-0.081273</td>\n",
       "      <td>2.311214</td>\n",
       "      <td>-0.341197</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.543066</td>\n",
       "      <td>-0.498601</td>\n",
       "      <td>-0.428053</td>\n",
       "      <td>-0.425064</td>\n",
       "      <td>-0.281797</td>\n",
       "      <td>-0.418054</td>\n",
       "      <td>-0.275911</td>\n",
       "      <td>-0.448893</td>\n",
       "      <td>-0.452825</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.574043</td>\n",
       "      <td>-0.035934</td>\n",
       "      <td>-0.498843</td>\n",
       "      <td>-0.370913</td>\n",
       "      <td>-0.762005</td>\n",
       "      <td>-1.188512</td>\n",
       "      <td>0.141500</td>\n",
       "      <td>-0.491242</td>\n",
       "      <td>-0.957426</td>\n",
       "      <td>0.322406</td>\n",
       "      <td>...</td>\n",
       "      <td>2.586924</td>\n",
       "      <td>-0.498601</td>\n",
       "      <td>-0.428053</td>\n",
       "      <td>-0.437894</td>\n",
       "      <td>-0.281797</td>\n",
       "      <td>-0.418054</td>\n",
       "      <td>-0.275911</td>\n",
       "      <td>0.462756</td>\n",
       "      <td>-0.452825</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.337845</td>\n",
       "      <td>1.094775</td>\n",
       "      <td>1.275121</td>\n",
       "      <td>0.353824</td>\n",
       "      <td>1.394463</td>\n",
       "      <td>1.272303</td>\n",
       "      <td>1.201621</td>\n",
       "      <td>0.452905</td>\n",
       "      <td>0.969756</td>\n",
       "      <td>-0.265192</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.058193</td>\n",
       "      <td>-0.498601</td>\n",
       "      <td>-0.428053</td>\n",
       "      <td>-0.427879</td>\n",
       "      <td>-0.281797</td>\n",
       "      <td>-0.418054</td>\n",
       "      <td>-0.275911</td>\n",
       "      <td>-0.166442</td>\n",
       "      <td>-0.452825</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.350920</td>\n",
       "      <td>0.777918</td>\n",
       "      <td>-0.726840</td>\n",
       "      <td>-0.223558</td>\n",
       "      <td>-1.016596</td>\n",
       "      <td>-0.926821</td>\n",
       "      <td>0.527291</td>\n",
       "      <td>-0.792252</td>\n",
       "      <td>-0.877961</td>\n",
       "      <td>0.511171</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.543066</td>\n",
       "      <td>-0.498601</td>\n",
       "      <td>-0.428053</td>\n",
       "      <td>-0.448061</td>\n",
       "      <td>-0.281797</td>\n",
       "      <td>-0.418054</td>\n",
       "      <td>-0.275911</td>\n",
       "      <td>-0.448893</td>\n",
       "      <td>-0.392866</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>54 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          $$        $_        $a        $b        $c        $d        $e  \\\n",
       "17  0.416581  0.724355  0.415723 -0.678734 -0.973332 -0.650595 -0.396620   \n",
       "5   1.120851  0.754248 -0.134649 -0.331625 -0.658614 -0.085545 -0.823966   \n",
       "33 -0.325331 -0.585072 -1.236515  1.877263 -0.211181 -0.116645 -0.431707   \n",
       "48 -1.079756  0.965223  0.604572 -0.770761 -0.413110 -0.474208 -0.162656   \n",
       "58 -1.162418 -1.480115  1.662141 -0.692058  0.422829 -0.342705 -0.757610   \n",
       "2   0.479922 -0.069024 -1.016872 -0.317584 -0.651994 -0.889686 -0.865250   \n",
       "36 -0.419403 -0.629671 -0.245953  0.937869 -0.465707  0.939226 -0.617174   \n",
       "54  0.084788 -0.117571  0.370100 -0.561941  0.821552 -0.558541  0.326848   \n",
       "46 -0.870141  1.050332  0.771331 -0.120226 -0.119737  0.866913 -0.414991   \n",
       "51 -0.585015 -0.009095 -0.342072 -0.744674 -0.604092 -0.743825 -0.442110   \n",
       "59  0.160193 -0.511410  1.069538  0.386715  1.337398 -0.555819  0.504810   \n",
       "35 -0.607005 -0.789282 -0.789932  1.403177  0.105876  0.858891 -0.563635   \n",
       "32 -0.731713 -1.069508 -0.770870  0.943741 -0.165573  1.359172  0.163119   \n",
       "10  0.374859  0.792281 -0.885054 -0.102468 -0.338211 -0.562495 -0.732605   \n",
       "56  0.471188  0.406604  0.773766 -0.493871  0.056359 -0.479218 -0.111865   \n",
       "39 -0.130612 -0.433647 -0.906393  2.334391 -0.069298  1.575522 -0.336380   \n",
       "8   0.472022  0.478789 -0.955261  1.189247 -0.155047  0.017002 -0.534238   \n",
       "40  0.212116 -0.388959 -1.012269  1.266830 -0.138706  0.496486 -0.560331   \n",
       "19 -0.242526 -0.034328 -0.869410 -0.778517 -1.070747 -0.594863 -0.658519   \n",
       "31  0.142429  0.238991  0.939711  1.545391  0.998756  3.126071  1.689346   \n",
       "28  0.146130  1.348289  1.810425 -0.026592  2.606196  2.133229  1.099025   \n",
       "49 -0.702308 -0.345426 -0.596958 -0.645196 -0.335803 -0.608656 -0.343109   \n",
       "30 -0.985567 -0.626714  1.241368 -0.733494  1.978165  1.399586  1.333401   \n",
       "38 -0.192614 -1.926658 -0.985484  2.868378 -0.495550 -0.317526 -0.415041   \n",
       "18 -0.513664  0.665937 -0.711762 -0.946432 -1.079040 -0.186931 -0.582588   \n",
       "29 -0.246974  1.116871  1.936067 -0.541777  1.698733  2.109133  1.623385   \n",
       "43 -0.501468  0.004917 -0.115528 -0.897042 -0.396359 -0.834175 -0.436480   \n",
       "34 -0.274015 -1.322232 -1.071332  2.319960  0.991819  0.615340 -0.273673   \n",
       "0   1.606613  1.290868 -1.151992 -0.850743 -0.865548 -0.941448 -0.934557   \n",
       "20  0.180615  0.166149 -0.930366  1.191977 -0.832784 -0.435478 -0.519567   \n",
       "37 -1.160164 -1.146230 -0.422590  1.920241 -0.209121  0.397336 -0.501577   \n",
       "3   0.615245  0.895433 -1.055850 -0.412176 -0.708640 -0.709287 -0.890224   \n",
       "47 -1.131557 -1.189601  0.114841 -0.950014 -0.951953 -0.966449 -0.527125   \n",
       "57 -1.252685 -1.505715  1.496289 -0.804829 -0.440550 -0.607955 -0.882890   \n",
       "13  5.740946 -0.227077 -0.032320 -0.797653 -0.938446 -1.135772  0.093860   \n",
       "22  0.700461  2.503420  0.690355 -0.316830  0.857275  0.392512  4.008051   \n",
       "44  0.076709  1.156331 -0.273990 -0.740141 -0.671521 -0.509198 -0.566176   \n",
       "24 -0.794710 -0.443708  2.248452 -0.088203  1.191352  0.305211  1.907300   \n",
       "21  0.414358 -0.442215 -1.126501 -1.001828 -0.762116 -0.290679 -0.280082   \n",
       "23  0.193774  1.395213  1.803825 -0.143628  2.903953  2.254735  3.313954   \n",
       "45  0.126604  0.753360 -0.034665 -0.962101 -0.234993 -0.729560 -0.152465   \n",
       "53 -1.058618 -1.816994  1.543473 -0.881029 -0.033405 -0.567117 -0.517968   \n",
       "9   1.222231  1.226280 -1.005652 -0.721303 -0.194072  1.367404 -0.155472   \n",
       "50 -0.353525  0.271025 -0.123316  0.288610 -0.183117  0.305245 -0.240993   \n",
       "7   0.164446 -0.233160 -1.025623 -0.627827 -0.649142 -0.875562 -0.861011   \n",
       "1   0.094701 -0.632283 -0.955982 -0.671482 -0.630449 -0.515897 -0.665854   \n",
       "55 -0.988473 -1.213568  0.729079 -0.863401 -0.004365 -0.655731 -0.199782   \n",
       "6  -0.270592 -0.777777 -0.689856 -0.490257 -0.707406 -0.679490 -0.786643   \n",
       "11 -0.467117 -1.502584 -0.694044 -0.812725 -1.122372 -1.153273 -0.486382   \n",
       "52 -0.212479 -0.633552  1.046139  0.748302  0.244387 -0.215460 -0.173398   \n",
       "27 -0.784760  0.085900  1.812258 -0.292717  2.393790  0.475154  1.434863   \n",
       "14  0.574043 -0.035934 -0.498843 -0.370913 -0.762005 -1.188512  0.141500   \n",
       "25 -0.337845  1.094775  1.275121  0.353824  1.394463  1.272303  1.201621   \n",
       "15  0.350920  0.777918 -0.726840 -0.223558 -1.016596 -0.926821  0.527291   \n",
       "\n",
       "          $f        $g        $h  ...         zr        zs        zt  \\\n",
       "17 -0.943649 -0.864329  0.840628  ...  -0.543066 -0.498601 -0.428053   \n",
       "5  -0.596442 -0.633240 -0.321978  ...  -0.543066 -0.498601 -0.335196   \n",
       "33 -0.030080  0.331097  0.853819  ...   0.710842  1.221637  2.202934   \n",
       "48 -0.937676 -0.877084 -1.127069  ...  -0.543066 -0.498601 -0.428053   \n",
       "58  4.230831  1.265686 -1.002417  ...  -0.543066 -0.498601 -0.428053   \n",
       "2  -0.604430 -0.915983 -0.875618  ...  -0.089210 -0.498601 -0.428053   \n",
       "36  0.122534  0.615506  1.581265  ...   2.391855  0.508005  1.453602   \n",
       "54 -0.428954  0.236760 -0.813851  ...  -0.543066  0.051543 -0.428053   \n",
       "46 -0.356353 -0.508410 -0.345873  ...   0.986294 -0.498601 -0.428053   \n",
       "51 -0.326296 -0.867174 -1.005133  ...  -0.543066 -0.498601 -0.428053   \n",
       "59  0.818337 -0.460647 -0.895661  ...  -0.543066 -0.498601 -0.428053   \n",
       "35  0.437168  0.305471  1.246306  ...   0.308898  1.839021  3.916848   \n",
       "32  0.289471  0.656157  3.330577  ...   1.708625  1.045949  1.278041   \n",
       "10  2.286174 -0.509960 -0.707194  ...  -0.543066 -0.498601 -0.428053   \n",
       "56  0.779181 -0.428327 -0.742011  ...  -0.543066 -0.498601 -0.428053   \n",
       "39  0.797473  0.605589  1.578526  ...   2.401255  4.550550  1.459628   \n",
       "8  -0.164209 -0.110410 -0.500498  ...  -0.543066  1.575044 -0.369321   \n",
       "40  0.162889  0.699592  1.402485  ...   1.341123  0.147630  2.372317   \n",
       "19 -0.825305 -1.081411  1.818419  ...  -0.543066 -0.498601 -0.428053   \n",
       "31  0.143515  2.817628  0.960311  ...  -0.543066 -0.498601 -0.428053   \n",
       "28  2.254320  1.173588  0.034305  ...   0.750949 -0.498601 -0.428053   \n",
       "49 -0.259409 -0.810021 -0.918792  ...  -0.543066 -0.498601 -0.428053   \n",
       "30  0.696612  1.197366  0.443793  ...  -0.543066 -0.498601 -0.428053   \n",
       "38  2.220213  1.314527  0.568383  ...   3.622817  2.358992  2.242804   \n",
       "18 -0.646024 -0.989753  0.730740  ...  -0.543066  0.580242 -0.428053   \n",
       "29  0.512615  0.969572 -0.167648  ...  -0.543066 -0.048653 -0.428053   \n",
       "43 -0.779942 -0.916704 -0.769088  ...  -0.152984 -0.498601 -0.405317   \n",
       "34  0.049977  0.551903  1.345466  ...   2.558575  1.628973  2.645150   \n",
       "0  -0.806296 -0.901088 -0.929057  ...   0.165733 -0.012400 -0.428053   \n",
       "20 -1.078332 -0.948434  0.920561  ...  -0.543066 -0.498601 -0.428053   \n",
       "37 -0.675719  1.334511  0.805749  ...   0.210705  1.569597  2.032189   \n",
       "3  -0.672407 -0.352244 -0.856441  ...  -0.008425 -0.498601 -0.428053   \n",
       "47 -0.953242 -0.667235 -1.057044  ...  -0.543066 -0.102647 -0.428053   \n",
       "57  0.252852 -0.559621 -1.092905  ...  -0.543066 -0.069201 -0.428053   \n",
       "13 -0.975889 -1.026972  1.888929  ...  -0.543066 -0.498601 -0.428053   \n",
       "22  2.059705  0.563204  0.596550  ...  -0.543066 -0.498601 -0.428053   \n",
       "44 -0.636286 -0.208105 -0.984900  ...   0.604730 -0.498601 -0.428053   \n",
       "24  0.160505  1.267693 -0.307165  ...  -0.543066 -0.498601 -0.396443   \n",
       "21  0.326404 -1.038486 -0.159550  ...  -0.543066 -0.498601 -0.428053   \n",
       "23  0.261266  2.307390  0.675373  ...  -0.543066 -0.498601 -0.428053   \n",
       "45 -1.055616 -0.489111 -0.836700  ...  -0.543066 -0.498601 -0.428053   \n",
       "53 -0.496778 -0.299887 -1.049161  ...  -0.543066 -0.498601 -0.428053   \n",
       "9   0.343983 -0.063664 -0.527539  ...  -0.543066 -0.498601 -0.428053   \n",
       "50 -0.363879 -0.621295 -0.719784  ...  -0.543066 -0.498601 -0.428053   \n",
       "7  -0.743711 -0.700612 -0.661963  ...  -0.543066 -0.498601 -0.428053   \n",
       "1  -0.773050 -0.680659 -0.891426  ...   0.288623 -0.498601 -0.428053   \n",
       "55 -0.582613 -0.152036 -0.958272  ...  -0.543066 -0.498601 -0.428053   \n",
       "6  -0.616063 -0.712942 -0.783167  ...  -0.543066  2.764593 -0.428053   \n",
       "11 -1.154678 -0.843757  0.422122  ...  -0.543066 -0.498601 -0.428053   \n",
       "52  1.172185  0.146983 -0.746660  ...  -0.543066 -0.498601 -0.325594   \n",
       "27 -0.081273  2.311214 -0.341197  ...  -0.543066 -0.498601 -0.428053   \n",
       "14 -0.491242 -0.957426  0.322406  ...   2.586924 -0.498601 -0.428053   \n",
       "25  0.452905  0.969756 -0.265192  ...  -0.058193 -0.498601 -0.428053   \n",
       "15 -0.792252 -0.877961  0.511171  ...  -0.543066 -0.498601 -0.428053   \n",
       "\n",
       "          zu        zv        zw        zx        zy        zz  bias  \n",
       "17 -0.440921 -0.281797 -0.418054 -0.275911 -0.448893 -0.452825   1.0  \n",
       "5  -0.443902 -0.281797 -0.418054 -0.275911  0.247155 -0.427383   1.0  \n",
       "33  2.650994 -0.281797  2.054151 -0.275911  0.646756 -0.332681   1.0  \n",
       "48 -0.453571 -0.281797 -0.418054 -0.275911 -0.448893  1.422711   1.0  \n",
       "58 -0.412307 -0.281797 -0.418054 -0.275911 -0.448893 -0.452825   1.0  \n",
       "2  -0.433828  0.414915 -0.404073 -0.275911 -0.052319 -0.452825   1.0  \n",
       "36  2.812156  1.970898  1.932708 -0.275911  1.260772 -0.405957   1.0  \n",
       "54 -0.450279 -0.281797 -0.418054 -0.275911 -0.448893 -0.375979   1.0  \n",
       "46 -0.438472 -0.281797 -0.418054 -0.275911 -0.003447  1.256774   1.0  \n",
       "51 -0.463298 -0.281797 -0.418054  2.072622 -0.448893  4.015987   1.0  \n",
       "59 -0.296696 -0.281797 -0.418054 -0.275911 -0.448893 -0.452825   1.0  \n",
       "35  2.721069  1.026050  4.726126 -0.275911  0.295542 -0.384799   1.0  \n",
       "32  1.748099 -0.281797  1.940397 -0.275911  2.830273 -0.237077   1.0  \n",
       "10 -0.365732  1.137658 -0.418054  1.633115 -0.179572 -0.423293   1.0  \n",
       "56 -0.422477 -0.281797 -0.418054 -0.275911 -0.448893 -0.399281   1.0  \n",
       "39  1.568013  3.108068  1.668127 -0.275911 -0.020107 -0.382297   1.0  \n",
       "8  -0.414225 -0.281797 -0.262841 -0.275911 -0.448893 -0.388458   1.0  \n",
       "40  2.197710  1.164410  0.800887 -0.275911  0.648694 -0.392647   1.0  \n",
       "19 -0.463298 -0.281797 -0.418054 -0.275911 -0.448893 -0.452825   1.0  \n",
       "31 -0.404733 -0.281797 -0.362484 -0.275911 -0.448893 -0.452825   1.0  \n",
       "28 -0.442293 -0.281797 -0.418054 -0.275911 -0.448893 -0.060199   1.0  \n",
       "49 -0.463298 -0.281797 -0.418054 -0.275911 -0.448893  1.200665   1.0  \n",
       "30 -0.463298 -0.281797 -0.418054 -0.275911 -0.448893 -0.452825   1.0  \n",
       "38  2.816532  6.113241  1.635311 -0.275911  1.977838 -0.319773   1.0  \n",
       "18 -0.463298 -0.281797 -0.418054 -0.275911 -0.448893 -0.452825   1.0  \n",
       "29 -0.457974 -0.281797 -0.418054 -0.275911 -0.448893 -0.431875   1.0  \n",
       "43 -0.437970 -0.281797 -0.418054 -0.275911 -0.108045  1.316306   1.0  \n",
       "34  1.626200 -0.281797  1.301845 -0.275911  0.454499 -0.353763   1.0  \n",
       "0  -0.440286 -0.281797 -0.418054 -0.275911 -0.448893 -0.430187   1.0  \n",
       "20 -0.338119 -0.281797 -0.374862 -0.275911 -0.448893 -0.452825   1.0  \n",
       "37  2.326522 -0.281797  2.066576 -0.275911 -0.009803 -0.356528   1.0  \n",
       "3  -0.450280 -0.281797 -0.418054 -0.275911 -0.137451 -0.452825   1.0  \n",
       "47 -0.458613 -0.281797 -0.418054 -0.275911 -0.280766  3.547799   1.0  \n",
       "57 -0.382004 -0.281797 -0.398770 -0.275911 -0.448893 -0.452825   1.0  \n",
       "13 -0.438515 -0.281797 -0.418054 -0.275911 -0.448893 -0.452825   1.0  \n",
       "22 -0.316883 -0.281797 -0.418054  3.862177  0.426795 -0.452825   1.0  \n",
       "44 -0.435350 -0.281797 -0.418054 -0.275911 -0.114583  2.149968   1.0  \n",
       "24 -0.445691 -0.281797 -0.367932 -0.275911 -0.448893 -0.452825   1.0  \n",
       "21 -0.463298 -0.281797 -0.418054 -0.275911 -0.448893 -0.452825   1.0  \n",
       "23 -0.450895 -0.281797 -0.347438 -0.275911 -0.226341 -0.404017   1.0  \n",
       "45 -0.443632 -0.281797 -0.418054 -0.275911 -0.448893  1.172272   1.0  \n",
       "53 -0.438055 -0.281797 -0.418054 -0.275911 -0.448893 -0.452825   1.0  \n",
       "9  -0.431093 -0.281797 -0.295818 -0.275911  1.862513 -0.452825   1.0  \n",
       "50 -0.360794 -0.281797 -0.369423 -0.275911 -0.448893  1.916884   1.0  \n",
       "7  -0.455361 -0.281797 -0.418054 -0.275911 -0.448893 -0.390360   1.0  \n",
       "1  -0.436297 -0.281797 -0.418054 -0.275911  0.035587 -0.373136   1.0  \n",
       "55 -0.424817 -0.281797 -0.418054 -0.275911 -0.448893 -0.452825   1.0  \n",
       "6  -0.447854 -0.281797 -0.418054 -0.275911  0.105343 -0.422438   1.0  \n",
       "11 -0.449991 -0.281797 -0.367546 -0.275911 -0.448893 -0.452825   1.0  \n",
       "52 -0.406225 -0.281797 -0.418054 -0.275911 -0.448893 -0.452825   1.0  \n",
       "27 -0.425064 -0.281797 -0.418054 -0.275911 -0.448893 -0.452825   1.0  \n",
       "14 -0.437894 -0.281797 -0.418054 -0.275911  0.462756 -0.452825   1.0  \n",
       "25 -0.427879 -0.281797 -0.418054 -0.275911 -0.166442 -0.452825   1.0  \n",
       "15 -0.448061 -0.281797 -0.418054 -0.275911 -0.448893 -0.392866   1.0  \n",
       "\n",
       "[54 rows x 785 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_vector = np.ones([X_train.shape[0], 1])\n",
    "X_train['bias'] = bias_vector\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise weight matrix with small weights\n",
    "\n",
    "np.random.seed(seed=200)\n",
    "\n",
    "W = np.random.randn(X_train.shape[1], len(lb.classes_)) * 0.0001\n",
    "#W = np.zeros([X.shape[1], len(lb.classes_)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the dimensions are right. The dot product of a specific row from X_train and the weight matrix W constitutes a forward pass and calculates the score for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(785, 6)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54, 785)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.002751</td>\n",
       "      <td>0.001102</td>\n",
       "      <td>0.000353</td>\n",
       "      <td>-0.000783</td>\n",
       "      <td>0.003843</td>\n",
       "      <td>-0.000011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5\n",
       "2 -0.002751  0.001102  0.000353 -0.000783  0.003843 -0.000011"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[5:6].dot(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the values for the highest score of the dot product is not the score of the true label. Our aim is to change this by implementing a support vector classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    0.003843\n",
       "dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[5:6].dot(W).max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    0.001102\n",
       "Name: 1, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[5:6].dot(W)[y_train[5:6].argmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important: we follow kind of a naive implementation. The aim is to be able to understand what is going on!\n",
    "\n",
    "In order to quantify how good (or how bad) our weight matrix W can predict the data in our training set, we need to implement a loss function. Here we take a go at the hinge loss, which tries to predict the correct class with a margin of at least one to all other classes (or in this case, like presented in Goldberg, to the class which does not equal the true class, but which scores highest). In my understanding, this is a one-vs-one approach (true class vs. class with highest score (but doesn't equal the true class))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_hinge_loss(x, y, W, index):\n",
    "    \"\"\"\n",
    "    Calculates the loss of a single data point by taking the prediction of the correct value and the the prediction of\n",
    "    the value of next highest score, following Crammer and Singer (2001)\n",
    "    :param x: sample point x as a vector\n",
    "    :param y: correct label y for x as a vector\n",
    "    :param W: weight matrix\n",
    "    :param index: column index of data matrix X\n",
    "    :return: loss\n",
    "    \"\"\"\n",
    "    loss = 0\n",
    "    y_index = y[index].argmax()\n",
    "    y_value = x.dot(W)[y_index]\n",
    "    y_hat_max_value = np.delete(x.dot(W), y_index).max()\n",
    "    #for j in range(0, y.shape[1]):  # in case we wanted to classify against all other classes (one-vs-all) --> currently one-vs-one\n",
    "        #if j == y_index:\n",
    "            #continue\n",
    "    loss += max(0, 1 - (y_value - y_hat_max_value))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With matrix multiplication, we could get all the scores at once. In the following, however, we focus on an approach which takes sample by sample and calculates the loss and the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = X_train.dot(W)  # simple matrix multiplication to get all scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.007375</td>\n",
       "      <td>-0.005238</td>\n",
       "      <td>-0.004242</td>\n",
       "      <td>-0.006294</td>\n",
       "      <td>0.000810</td>\n",
       "      <td>0.003542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.005797</td>\n",
       "      <td>-0.006754</td>\n",
       "      <td>0.001188</td>\n",
       "      <td>-0.000472</td>\n",
       "      <td>0.000874</td>\n",
       "      <td>-0.001011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.003550</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>-0.001113</td>\n",
       "      <td>-0.000362</td>\n",
       "      <td>-0.000552</td>\n",
       "      <td>0.003478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.002508</td>\n",
       "      <td>0.002688</td>\n",
       "      <td>0.002068</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>-0.000161</td>\n",
       "      <td>-0.002079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.000666</td>\n",
       "      <td>-0.000814</td>\n",
       "      <td>0.006181</td>\n",
       "      <td>0.002919</td>\n",
       "      <td>-0.002478</td>\n",
       "      <td>0.000340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.002751</td>\n",
       "      <td>0.001102</td>\n",
       "      <td>0.000353</td>\n",
       "      <td>-0.000783</td>\n",
       "      <td>0.003843</td>\n",
       "      <td>-0.000011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.000469</td>\n",
       "      <td>-0.005094</td>\n",
       "      <td>-0.002870</td>\n",
       "      <td>0.004014</td>\n",
       "      <td>0.001441</td>\n",
       "      <td>-0.000630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>-0.001155</td>\n",
       "      <td>0.004902</td>\n",
       "      <td>0.003001</td>\n",
       "      <td>0.005857</td>\n",
       "      <td>-0.001278</td>\n",
       "      <td>0.004642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>-0.002020</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.001625</td>\n",
       "      <td>-0.000914</td>\n",
       "      <td>-0.001955</td>\n",
       "      <td>-0.004747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>-0.000868</td>\n",
       "      <td>0.004471</td>\n",
       "      <td>0.001188</td>\n",
       "      <td>0.001433</td>\n",
       "      <td>0.001760</td>\n",
       "      <td>-0.004693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.006008</td>\n",
       "      <td>0.003144</td>\n",
       "      <td>0.001725</td>\n",
       "      <td>0.000498</td>\n",
       "      <td>0.003561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.006537</td>\n",
       "      <td>0.003965</td>\n",
       "      <td>-0.000181</td>\n",
       "      <td>0.004006</td>\n",
       "      <td>-0.002289</td>\n",
       "      <td>0.003331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.004173</td>\n",
       "      <td>0.001138</td>\n",
       "      <td>-0.004267</td>\n",
       "      <td>0.001704</td>\n",
       "      <td>-0.004642</td>\n",
       "      <td>-0.000215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000330</td>\n",
       "      <td>0.003580</td>\n",
       "      <td>0.001620</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>-0.002154</td>\n",
       "      <td>-0.000525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>-0.000096</td>\n",
       "      <td>-0.003043</td>\n",
       "      <td>0.002565</td>\n",
       "      <td>0.001790</td>\n",
       "      <td>0.001229</td>\n",
       "      <td>0.001621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.004419</td>\n",
       "      <td>-0.003623</td>\n",
       "      <td>-0.001976</td>\n",
       "      <td>-0.002775</td>\n",
       "      <td>-0.004207</td>\n",
       "      <td>0.003726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.004452</td>\n",
       "      <td>-0.003346</td>\n",
       "      <td>-0.002254</td>\n",
       "      <td>-0.001060</td>\n",
       "      <td>-0.003006</td>\n",
       "      <td>0.001828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.006370</td>\n",
       "      <td>0.001301</td>\n",
       "      <td>0.001286</td>\n",
       "      <td>0.003910</td>\n",
       "      <td>-0.000171</td>\n",
       "      <td>0.000557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.001943</td>\n",
       "      <td>-0.004439</td>\n",
       "      <td>-0.003164</td>\n",
       "      <td>-0.003586</td>\n",
       "      <td>-0.001905</td>\n",
       "      <td>0.001905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>-0.000495</td>\n",
       "      <td>-0.000507</td>\n",
       "      <td>-0.002981</td>\n",
       "      <td>0.000621</td>\n",
       "      <td>-0.002633</td>\n",
       "      <td>-0.004004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.005996</td>\n",
       "      <td>-0.001005</td>\n",
       "      <td>0.001701</td>\n",
       "      <td>0.004428</td>\n",
       "      <td>-0.002311</td>\n",
       "      <td>-0.003605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.004737</td>\n",
       "      <td>0.003290</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.001069</td>\n",
       "      <td>0.000452</td>\n",
       "      <td>-0.003263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-0.001233</td>\n",
       "      <td>-0.002934</td>\n",
       "      <td>-0.000541</td>\n",
       "      <td>0.005039</td>\n",
       "      <td>-0.000876</td>\n",
       "      <td>-0.004264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.007141</td>\n",
       "      <td>-0.000806</td>\n",
       "      <td>0.004739</td>\n",
       "      <td>0.002330</td>\n",
       "      <td>0.000795</td>\n",
       "      <td>0.005388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.002556</td>\n",
       "      <td>-0.002651</td>\n",
       "      <td>-0.001387</td>\n",
       "      <td>-0.004750</td>\n",
       "      <td>-0.002221</td>\n",
       "      <td>0.002117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.000909</td>\n",
       "      <td>-0.000145</td>\n",
       "      <td>-0.000128</td>\n",
       "      <td>0.004361</td>\n",
       "      <td>-0.001605</td>\n",
       "      <td>-0.002270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>-0.001228</td>\n",
       "      <td>0.000771</td>\n",
       "      <td>-0.001274</td>\n",
       "      <td>0.001021</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>-0.002370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.004879</td>\n",
       "      <td>-0.002489</td>\n",
       "      <td>0.002692</td>\n",
       "      <td>-0.003645</td>\n",
       "      <td>-0.002106</td>\n",
       "      <td>0.004787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.000345</td>\n",
       "      <td>0.001731</td>\n",
       "      <td>0.003518</td>\n",
       "      <td>-0.000687</td>\n",
       "      <td>-0.000464</td>\n",
       "      <td>0.000043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.001597</td>\n",
       "      <td>-0.005419</td>\n",
       "      <td>-0.005623</td>\n",
       "      <td>-0.005981</td>\n",
       "      <td>-0.001541</td>\n",
       "      <td>0.000970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.003906</td>\n",
       "      <td>-0.001544</td>\n",
       "      <td>0.004766</td>\n",
       "      <td>-0.000059</td>\n",
       "      <td>0.000457</td>\n",
       "      <td>0.001579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.002662</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>-0.001753</td>\n",
       "      <td>-0.001868</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.002518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>-0.001257</td>\n",
       "      <td>0.003793</td>\n",
       "      <td>0.000953</td>\n",
       "      <td>0.004190</td>\n",
       "      <td>0.001069</td>\n",
       "      <td>-0.001286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.002595</td>\n",
       "      <td>0.002539</td>\n",
       "      <td>0.001888</td>\n",
       "      <td>0.003167</td>\n",
       "      <td>0.000698</td>\n",
       "      <td>0.001879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.004556</td>\n",
       "      <td>-0.000559</td>\n",
       "      <td>-0.004094</td>\n",
       "      <td>-0.002580</td>\n",
       "      <td>0.000768</td>\n",
       "      <td>-0.001028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.000691</td>\n",
       "      <td>0.003455</td>\n",
       "      <td>0.005865</td>\n",
       "      <td>0.002181</td>\n",
       "      <td>0.001424</td>\n",
       "      <td>-0.004283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>-0.000615</td>\n",
       "      <td>0.003097</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.002293</td>\n",
       "      <td>0.002192</td>\n",
       "      <td>-0.003450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.001847</td>\n",
       "      <td>-0.000112</td>\n",
       "      <td>-0.000475</td>\n",
       "      <td>0.002385</td>\n",
       "      <td>-0.003065</td>\n",
       "      <td>-0.000987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.002691</td>\n",
       "      <td>-0.002476</td>\n",
       "      <td>-0.003640</td>\n",
       "      <td>-0.006247</td>\n",
       "      <td>0.000921</td>\n",
       "      <td>-0.003533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.000287</td>\n",
       "      <td>-0.000461</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>0.005086</td>\n",
       "      <td>-0.003822</td>\n",
       "      <td>-0.003455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.001827</td>\n",
       "      <td>0.001816</td>\n",
       "      <td>0.001158</td>\n",
       "      <td>-0.002593</td>\n",
       "      <td>0.004373</td>\n",
       "      <td>-0.003295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.000477</td>\n",
       "      <td>0.001624</td>\n",
       "      <td>0.002807</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>0.000728</td>\n",
       "      <td>0.001124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.001176</td>\n",
       "      <td>0.003629</td>\n",
       "      <td>0.004189</td>\n",
       "      <td>-0.004924</td>\n",
       "      <td>0.000870</td>\n",
       "      <td>0.000644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.001925</td>\n",
       "      <td>-0.000951</td>\n",
       "      <td>0.000402</td>\n",
       "      <td>-0.000302</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>-0.003073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000992</td>\n",
       "      <td>-0.001961</td>\n",
       "      <td>0.002037</td>\n",
       "      <td>0.003676</td>\n",
       "      <td>0.001697</td>\n",
       "      <td>0.004348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.001443</td>\n",
       "      <td>-0.004395</td>\n",
       "      <td>0.001517</td>\n",
       "      <td>0.002170</td>\n",
       "      <td>0.002341</td>\n",
       "      <td>0.000618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.000720</td>\n",
       "      <td>0.003837</td>\n",
       "      <td>0.002248</td>\n",
       "      <td>-0.000664</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.001701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.000799</td>\n",
       "      <td>-0.001324</td>\n",
       "      <td>0.001847</td>\n",
       "      <td>0.001653</td>\n",
       "      <td>0.003898</td>\n",
       "      <td>0.000179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.004031</td>\n",
       "      <td>-0.004317</td>\n",
       "      <td>-0.006703</td>\n",
       "      <td>-0.005073</td>\n",
       "      <td>0.001569</td>\n",
       "      <td>0.004237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.000392</td>\n",
       "      <td>0.002851</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.005175</td>\n",
       "      <td>-0.001648</td>\n",
       "      <td>-0.001481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.002820</td>\n",
       "      <td>0.000696</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>-0.001504</td>\n",
       "      <td>0.000833</td>\n",
       "      <td>-0.000193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.002860</td>\n",
       "      <td>-0.001570</td>\n",
       "      <td>-0.004666</td>\n",
       "      <td>-0.004958</td>\n",
       "      <td>0.003121</td>\n",
       "      <td>0.001371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.002664</td>\n",
       "      <td>0.000252</td>\n",
       "      <td>-0.001738</td>\n",
       "      <td>0.000594</td>\n",
       "      <td>-0.000996</td>\n",
       "      <td>-0.002293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.002085</td>\n",
       "      <td>-0.002386</td>\n",
       "      <td>-0.004894</td>\n",
       "      <td>-0.005947</td>\n",
       "      <td>0.002342</td>\n",
       "      <td>0.002372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5\n",
       "17 -0.007375 -0.005238 -0.004242 -0.006294  0.000810  0.003542\n",
       "5  -0.005797 -0.006754  0.001188 -0.000472  0.000874 -0.001011\n",
       "33  0.003550  0.001359 -0.001113 -0.000362 -0.000552  0.003478\n",
       "48  0.002508  0.002688  0.002068  0.000299 -0.000161 -0.002079\n",
       "58  0.000666 -0.000814  0.006181  0.002919 -0.002478  0.000340\n",
       "2  -0.002751  0.001102  0.000353 -0.000783  0.003843 -0.000011\n",
       "36  0.000469 -0.005094 -0.002870  0.004014  0.001441 -0.000630\n",
       "54 -0.001155  0.004902  0.003001  0.005857 -0.001278  0.004642\n",
       "46 -0.002020  0.000084  0.001625 -0.000914 -0.001955 -0.004747\n",
       "51 -0.000868  0.004471  0.001188  0.001433  0.001760 -0.004693\n",
       "59  0.000071  0.006008  0.003144  0.001725  0.000498  0.003561\n",
       "35  0.006537  0.003965 -0.000181  0.004006 -0.002289  0.003331\n",
       "32  0.004173  0.001138 -0.004267  0.001704 -0.004642 -0.000215\n",
       "10  0.000330  0.003580  0.001620  0.000101 -0.002154 -0.000525\n",
       "56 -0.000096 -0.003043  0.002565  0.001790  0.001229  0.001621\n",
       "39  0.004419 -0.003623 -0.001976 -0.002775 -0.004207  0.003726\n",
       "8   0.004452 -0.003346 -0.002254 -0.001060 -0.003006  0.001828\n",
       "40  0.006370  0.001301  0.001286  0.003910 -0.000171  0.000557\n",
       "19 -0.001943 -0.004439 -0.003164 -0.003586 -0.001905  0.001905\n",
       "31 -0.000495 -0.000507 -0.002981  0.000621 -0.002633 -0.004004\n",
       "28 -0.005996 -0.001005  0.001701  0.004428 -0.002311 -0.003605\n",
       "49  0.004737  0.003290  0.000019  0.001069  0.000452 -0.003263\n",
       "30 -0.001233 -0.002934 -0.000541  0.005039 -0.000876 -0.004264\n",
       "38  0.007141 -0.000806  0.004739  0.002330  0.000795  0.005388\n",
       "18 -0.002556 -0.002651 -0.001387 -0.004750 -0.002221  0.002117\n",
       "29 -0.000909 -0.000145 -0.000128  0.004361 -0.001605 -0.002270\n",
       "43 -0.001228  0.000771 -0.001274  0.001021  0.000244 -0.002370\n",
       "34  0.004879 -0.002489  0.002692 -0.003645 -0.002106  0.004787\n",
       "0  -0.000345  0.001731  0.003518 -0.000687 -0.000464  0.000043\n",
       "20 -0.001597 -0.005419 -0.005623 -0.005981 -0.001541  0.000970\n",
       "37  0.003906 -0.001544  0.004766 -0.000059  0.000457  0.001579\n",
       "3  -0.002662  0.001000 -0.001753 -0.001868  0.000030  0.002518\n",
       "47 -0.001257  0.003793  0.000953  0.004190  0.001069 -0.001286\n",
       "57  0.002595  0.002539  0.001888  0.003167  0.000698  0.001879\n",
       "13 -0.004556 -0.000559 -0.004094 -0.002580  0.000768 -0.001028\n",
       "22  0.000691  0.003455  0.005865  0.002181  0.001424 -0.004283\n",
       "44 -0.000615  0.003097 -0.000029 -0.002293  0.002192 -0.003450\n",
       "24 -0.001847 -0.000112 -0.000475  0.002385 -0.003065 -0.000987\n",
       "21  0.002691 -0.002476 -0.003640 -0.006247  0.000921 -0.003533\n",
       "23 -0.000287 -0.000461  0.000143  0.005086 -0.003822 -0.003455\n",
       "45  0.001827  0.001816  0.001158 -0.002593  0.004373 -0.003295\n",
       "53  0.000477  0.001624  0.002807  0.000247  0.000728  0.001124\n",
       "9   0.001176  0.003629  0.004189 -0.004924  0.000870  0.000644\n",
       "50  0.001925 -0.000951  0.000402 -0.000302  0.000042 -0.003073\n",
       "7   0.000992 -0.001961  0.002037  0.003676  0.001697  0.004348\n",
       "1  -0.001443 -0.004395  0.001517  0.002170  0.002341  0.000618\n",
       "55  0.000720  0.003837  0.002248 -0.000664  0.000074  0.001701\n",
       "6  -0.000799 -0.001324  0.001847  0.001653  0.003898  0.000179\n",
       "11 -0.004031 -0.004317 -0.006703 -0.005073  0.001569  0.004237\n",
       "52  0.000392  0.002851  0.000961  0.005175 -0.001648 -0.001481\n",
       "27 -0.002820  0.000696  0.000299 -0.001504  0.000833 -0.000193\n",
       "14 -0.002860 -0.001570 -0.004666 -0.004958  0.003121  0.001371\n",
       "25 -0.002664  0.000252 -0.001738  0.000594 -0.000996 -0.002293\n",
       "15 -0.002085 -0.002386 -0.004894 -0.005947  0.002342  0.002372"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(X, y, W):\n",
    "    \"\"\"\n",
    "    compute the gradient\n",
    "    :param X: data matrix (train) \n",
    "    :param y: the corresponding \n",
    "    :param W: weight matrix\n",
    "    :return: loss and Jacobian dW with all gradients\n",
    "    \"\"\"\n",
    "    dW = np.zeros(W.shape)\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for index, x in enumerate(X.as_matrix()):\n",
    "        y_index = y[index].argmax()\n",
    "        y_value = x.dot(W)[y_index]\n",
    "        y_hat_max_value = np.delete(x.dot(W), y_index).max()\n",
    "        loss = max(0, 1 - (y_value - y_hat_max_value))\n",
    "        total_loss += loss\n",
    "        y_hat_max_index = np.delete(x.dot(W), y_index).argmax() + 1\n",
    "        dW[:, y_hat_max_index] += x.transpose()\n",
    "        dW[:, y_index] -= x.transpose()\n",
    "            \n",
    "    return total_loss, dW\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, W, eta, steps):\n",
    "    \"\"\"\n",
    "    Perform gradient descent for a number of times with a fixed learning rate eta\n",
    "    :param X: data matrix\n",
    "    :param y: labels\n",
    "    :param W: weight matrix\n",
    "    :param eta: learning rate\n",
    "    :param steps: number of times gradient descent should be performed\n",
    "    :return: learned representation matrix W_learned\n",
    "    \"\"\"\n",
    "    W_learned = W.copy()\n",
    "    \n",
    "    for step in range(0, steps):\n",
    "        loss, dW = gradient(X, y, W_learned)\n",
    "        W_learned = W_learned - eta * dW\n",
    "        print(loss)\n",
    "        \n",
    "    return W_learned\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54.20622198371133\n",
      "21.490525959054526\n",
      "0.07620527271920619\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "W_star = gradient_descent(X_train, y_train, W, eta=0.001, steps=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "Let's test if our learned representation of the data is any good at classifying the data in the test set. Of course we need the bias in our test set as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_vector_test = np.ones([X_test.shape[0], 1])\n",
    "X_test['bias'] = bias_vector_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n",
      "3 3\n",
      "3 3\n",
      "4 4\n",
      "0 0\n",
      "5 5\n"
     ]
    }
   ],
   "source": [
    "for index, x in enumerate(X_test.dot(W_star).as_matrix()):\n",
    "    pred = x.argmax()\n",
    "    true_label = y_test[index].argmax()\n",
    "    print(pred, true_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too bad! But Goldberg mentioned something about regularisation, so we should take this into account as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_reg(X, y, W, lam):\n",
    "    \"\"\"\n",
    "    compute the gradient\n",
    "    :param X: data matrix (train) \n",
    "    :param y: the corresponding \n",
    "    :param W: weight matrix\n",
    "    :param lam: reguliser lambda\n",
    "    :return: Jacobian dW with all gradients\n",
    "    \"\"\"\n",
    "    dW = np.zeros(W.shape)\n",
    "    \n",
    "    for index, x in enumerate(X.as_matrix()):\n",
    "        y_index = y[index].argmax()\n",
    "        y_value = x.dot(W)[y_index]\n",
    "        y_hat_max_value = np.delete(x.dot(W), y_index).max()\n",
    "        loss = max(0, 1 - (y_value - y_hat_max_value)) + lam * np.linalg.norm(W, 2)\n",
    "        y_hat_max_index = np.delete(x.dot(W), y_index).argmax() + 1\n",
    "        dW[:, y_hat_max_index] += x.transpose()\n",
    "        dW[:, y_index] -= x.transpose()\n",
    "        \n",
    "    dW += 2 * lam * W\n",
    "            \n",
    "    return dW\n",
    "\n",
    "def gradient_descent_reg(X, y, W, eta, steps):\n",
    "    \"\"\"\n",
    "    Perform gradient descent for a number of times with a fixed learning rate eta\n",
    "    :param X: data matrix\n",
    "    :param y: labels\n",
    "    :param W: weight matrix\n",
    "    :param eta: learning rate\n",
    "    :param steps: number of times gradient descent should be performed\n",
    "    :return: learned representation matrix W_learned\n",
    "    \"\"\"\n",
    "    W_learned = W.copy()\n",
    "    \n",
    "    for step in range(0, steps):\n",
    "        dW = gradient_reg(X, y, W, 10000)\n",
    "        W_learned = W_learned - eta * dW\n",
    "        \n",
    "    return W_learned\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_star_reg = gradient_descent_reg(X_train, y_train, W, eta=0.001, steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n",
      "3 3\n",
      "3 3\n",
      "5 4\n",
      "0 0\n",
      "5 5\n"
     ]
    }
   ],
   "source": [
    "for index, x in enumerate(X_test.dot(W_star_reg).as_matrix()):\n",
    "    pred = x.argmax()\n",
    "    true_label = y_test[index].argmax()\n",
    "    print(pred, true_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the two different weight matrices (one regularised, the other not), we notice the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -3.62888535,   6.04994232,  -2.86310234,  10.24142028,\n",
       "         -5.07025647,  -4.72899317],\n",
       "       [ -8.29128073,   3.76352834,  -8.44622223,   2.74631596,\n",
       "          5.51610756,   4.71172435],\n",
       "       [ -7.4413166 ,  -6.07465375,   7.14779523,  -9.16551201,\n",
       "          5.69735729,   9.83666962],\n",
       "       [ 15.87176373,  -3.16702729,   0.82967888,  -6.72842564,\n",
       "          3.885268  , -10.69136123],\n",
       "       [ -0.65747427,  -4.1231676 ,   2.66509709,  -9.73523486,\n",
       "          8.2896523 ,   3.56119809]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_star[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-7.26988850e-01,  2.22803605e+00, -4.28686065e+00,\n",
       "         1.07368969e+01, -5.79318894e+00, -4.66309383e+00],\n",
       "       [-7.84858729e+00,  2.80901479e+00, -7.06234349e+00,\n",
       "         1.16230410e+00,  5.36960929e+00,  2.10515229e+00],\n",
       "       [-7.86827958e+00, -8.10935165e+00,  3.32437167e+00,\n",
       "        -8.10616875e+00,  2.01308697e+00,  1.19511398e+01],\n",
       "       [ 1.75975963e+01, -3.64229029e+00,  3.13804242e+00,\n",
       "        -9.15839372e+00,  6.47278616e+00, -1.23368068e+01],\n",
       "       [ 6.82798763e-03, -1.56030958e+00,  2.22802146e+00,\n",
       "        -1.39021833e+01,  8.78608843e+00,  3.02627729e+00]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_star_reg[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By the way ...\n",
    "### In scikit-learn it's much easier to implement this :-)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='hinge', max_iter=1000,\n",
       "     multi_class='crammer_singer', penalty='l2', random_state=0,\n",
       "     tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC(random_state=0, multi_class='crammer_singer', loss='hinge')\n",
    "clf.fit(X_train, train.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['en', 'fi', 'fi', 'fr', 'de', 'it'], dtype=object)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4     en\n",
       "12    fi\n",
       "16    fi\n",
       "26    fr\n",
       "41    de\n",
       "42    it\n",
       "Name: y, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that with our naive implementation, we do not much worse than with scikit's. scikit's implementation is of course much more elaborate and uses the vectorised operation and possibly other optimisation techniques in order to make its SVM (or SVC) better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
