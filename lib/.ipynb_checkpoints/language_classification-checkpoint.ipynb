{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Class Classification - Language Classification\n",
    "\n",
    "This notebook implements the method presented in Goldberg's [2017] book \"Neural Network Methods for Natural Language Processing\". It shows the steps you need to go through in order to successfully train a classifier, and it should also, so I hope, illustrate the notational differences between Goldberg and standard machine learning literature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting and cleaning the data\n",
    "\n",
    "The data consists of downloaded Wikipedia articles about the Second World War in German, English, French, Spanish, Italian and Finnish (instead of \"O\" in Goldberg). The data is in HTML, so we need to some preprocessing to get the text out of it. We also restrict ourselfes to the characters from a to z in the alphabet (as described in Goldberg). In this fashion, we get rid of all the Umlauts (ä, ö, ü) and all other characters with diacritics (as, e.g., the é or ç in French). Note however, that if these characters ocurring in bigrams would probably be good features. In some way, we still keep the information \"special character\" by not fully deleting the character, but by replacing it by the dollar sign \"\\$\". Furthermore, we replace all punctuation marks and digits by dollar signs as well. As such, all special characters, digits, and punctuation marks are mapped to $. The space will be replaced by an underscore \"\\_\". We then represent each langauge by 28 characters, as is suggested by Goldberg."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning HTML\n",
    "We first strip the HTML to get only the text of the Wikipedia page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the html files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "from collections import defaultdict\n",
    "\n",
    "article_dict = defaultdict(lambda: defaultdict(str))\n",
    "\n",
    "regex = r'[\\n ]{2,}'\n",
    "pattern = re.compile(regex)\n",
    "\n",
    "urls = open('urls.txt', 'r').readlines()\n",
    "\n",
    "for index, url in enumerate(urls):\n",
    "    language = url[8:10]\n",
    "    doc_id = 'doc_%d' % index\n",
    "    html = urlopen(url.strip()).read()    \n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    raw = soup.body.get_text()  # only get text from the text body (this excludes headers and should exclude navigation bars)\n",
    "    raw = re.sub(pattern, ' ', raw)  # replace multiple breaks and spaces by only one space\n",
    "    raw = re.sub(r'\\n', ' ', raw)  # replace every line break with a space\n",
    "    article_dict[language][doc_id] = raw.lower()  # assign each text to its language and lower all uppercase characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing --> prepare the text\n",
    "replace special characters and digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_dict = defaultdict(lambda: defaultdict(str))\n",
    "\n",
    "abc = r'[a-z]'\n",
    "abc_pattern = re.compile(abc)\n",
    "\n",
    "for lang, doc in article_dict.items():\n",
    "    for doc, text in doc.items():\n",
    "        for char in text:\n",
    "            if re.match(abc_pattern, char):\n",
    "                preprocessed_dict[lang][doc] += char\n",
    "            elif re.match(' ', char):\n",
    "                preprocessed_dict[lang][doc] += '_'\n",
    "            else:\n",
    "                preprocessed_dict[lang][doc] += '$'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count bigrams --> Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of bigrams will be our only feature. We could extend this by taking into account other n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "charset = 'abcdefghijklmnopqrstuvwxyz$_'  # define the character set we want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations_with_replacement, permutations\n",
    "\n",
    "def bigrams(text):\n",
    "    \"\"\"\n",
    "    Function to extract bigrams from text and calculate their distribution\n",
    "    :param text: text string\n",
    "    :return: dictionary containing bigrams as keys, and the normalised count as values\n",
    "    \"\"\"\n",
    "    combs = combinations_with_replacement(charset, 2)\n",
    "    perms = permutations(charset, 2)\n",
    "    bigram_dict = dict()\n",
    "    \n",
    "    for comb in set(list(combs) + list(perms)):\n",
    "        bigram_dict[''.join(comb)] = 0\n",
    "        \n",
    "    doc_length = len(text)\n",
    "    \n",
    "    for index in range(0, len(text)-1):\n",
    "        bigram = text[index] + text[index+1]\n",
    "        bigram_dict[bigram] += 1\n",
    "                \n",
    "    for bigram, count in bigram_dict.items():\n",
    "        bigram_dict[bigram] = count/doc_length\n",
    "\n",
    "    return bigram_dict              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put data into pandas dataframe\n",
    "The pandas dataframe allows us to conveniently represent all the data we need in one table. So let's do this. But first we need to extract the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_dict_full = defaultdict(lambda: defaultdict(dict))\n",
    "\n",
    "for lang, doc in preprocessed_dict.items():\n",
    "    for doc, text in sorted(doc.items()):\n",
    "        bigram_dict = bigrams(text)\n",
    "        bigram_dict_full[lang][doc] = bigram_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>$$</th>\n",
       "      <th>$_</th>\n",
       "      <th>$a</th>\n",
       "      <th>$b</th>\n",
       "      <th>$c</th>\n",
       "      <th>$d</th>\n",
       "      <th>$e</th>\n",
       "      <th>$f</th>\n",
       "      <th>$g</th>\n",
       "      <th>$h</th>\n",
       "      <th>...</th>\n",
       "      <th>zq</th>\n",
       "      <th>zr</th>\n",
       "      <th>zs</th>\n",
       "      <th>zt</th>\n",
       "      <th>zu</th>\n",
       "      <th>zv</th>\n",
       "      <th>zw</th>\n",
       "      <th>zx</th>\n",
       "      <th>zy</th>\n",
       "      <th>zz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.068604</td>\n",
       "      <td>0.025286</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.000447</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.001341</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.001490</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.070415</td>\n",
       "      <td>0.034538</td>\n",
       "      <td>0.002064</td>\n",
       "      <td>0.000317</td>\n",
       "      <td>0.000249</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>0.001021</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.001066</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.053590</td>\n",
       "      <td>0.031392</td>\n",
       "      <td>0.000611</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.000320</td>\n",
       "      <td>0.000640</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.001571</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.047864</td>\n",
       "      <td>0.025301</td>\n",
       "      <td>0.000809</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000890</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000850</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.070379</td>\n",
       "      <td>0.029698</td>\n",
       "      <td>0.000320</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.000412</td>\n",
       "      <td>0.000458</td>\n",
       "      <td>0.001190</td>\n",
       "      <td>0.000549</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000549</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         $$        $_        $a        $b        $c        $d        $e  \\\n",
       "0  0.068604  0.025286  0.000944  0.000944  0.000447  0.000248  0.001341   \n",
       "1  0.070415  0.034538  0.002064  0.000317  0.000249  0.000295  0.001021   \n",
       "2  0.053590  0.031392  0.000611  0.000262  0.000175  0.000320  0.000640   \n",
       "3  0.047864  0.025301  0.000809  0.000243  0.000135  0.000067  0.000890   \n",
       "4  0.070379  0.029698  0.000320  0.000137  0.000412  0.000458  0.001190   \n",
       "\n",
       "         $f        $g        $h ...    zq   zr   zs   zt        zu   zv  \\\n",
       "0  0.000248  0.000099  0.001490 ...   0.0  0.0  0.0  0.0  0.000050  0.0   \n",
       "1  0.000136  0.000113  0.001066 ...   0.0  0.0  0.0  0.0  0.000023  0.0   \n",
       "2  0.000175  0.000029  0.001571 ...   0.0  0.0  0.0  0.0  0.000000  0.0   \n",
       "3  0.000067  0.000121  0.000850 ...   0.0  0.0  0.0  0.0  0.000013  0.0   \n",
       "4  0.000549  0.000046  0.000549 ...   0.0  0.0  0.0  0.0  0.000000  0.0   \n",
       "\n",
       "         zw   zx   zy   zz  \n",
       "0  0.000000  0.0  0.0  0.0  \n",
       "1  0.000000  0.0  0.0  0.0  \n",
       "2  0.000000  0.0  0.0  0.0  \n",
       "3  0.000013  0.0  0.0  0.0  \n",
       "4  0.000000  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "col_names = ['y'] + sorted(bigram_dict_full['en']['doc_0'].keys())\n",
    "my_df = dict()\n",
    "\n",
    "for col in col_names:\n",
    "    my_df[col] = list()\n",
    "    \n",
    "df = pd.DataFrame(my_df)\n",
    "\n",
    "for lang, doc in bigram_dict_full.items():\n",
    "    for key, value in doc.items():\n",
    "        df_obj = value\n",
    "        df_obj['y'] = lang\n",
    "        df = df.append(df_obj, ignore_index=True)\n",
    "        \n",
    "df.head()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 785)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split the data into the label vector \\begin{equation}\\mathbf{y}\\end{equation} and a training data matrix \\begin{equation}\\mathbf{X}\\end{equation}. But first, we shuffle the df and split it into a training and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df.sample(frac=0.9,random_state=200)\n",
    "test = df.drop(train.index)\n",
    "\n",
    "y = train.y\n",
    "X = train.drop('y', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54, 784)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999709065518464"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(X.iloc[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
